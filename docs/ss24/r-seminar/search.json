[
  {
    "objectID": "cheatsheets.html",
    "href": "cheatsheets.html",
    "title": "Cheatsheets",
    "section": "",
    "text": "Hier sind die Cheatsheets von posit.co zu finden:"
  },
  {
    "objectID": "basics.html",
    "href": "basics.html",
    "title": "Basics",
    "section": "",
    "text": "Student, der endlich versteht was R & R-Studio ist",
    "crumbs": [
      "Home",
      "Program",
      "Basics"
    ]
  },
  {
    "objectID": "basics.html#was-ist-r",
    "href": "basics.html#was-ist-r",
    "title": "Basics",
    "section": "Was ist R?",
    "text": "Was ist R?\nR ist eine Programmiersprache (und mit R-Studio auch eine Programmieroberfläche) für statistische Berechnungen und Grafiken. Es ist Open-Source & kostenlos, und durch die große Community, die ständig wächst, wird es stets um Funktionen erweitert. Die Community kann auch bei Fragen helfen. Die meistgenutzte IDE ist R-Studio, welches die Handhabung wesentlich einfacher macht.\nDie Besonderheit bei R ist, dass man besonders nah an den Daten dran ist, und dementsprechend schnell explorative Analysen machen oder Plots erstellen kann. Diese Nähe schreckt manche vielleicht zu Beginn ab - langfristig kann sie sich aber als großer Vorteil erweisen.",
    "crumbs": [
      "Home",
      "Program",
      "Basics"
    ]
  },
  {
    "objectID": "basics.html#user-interface",
    "href": "basics.html#user-interface",
    "title": "Basics",
    "section": "User Interface",
    "text": "User Interface\nIn den meisten Fällen benutzen wir R über unsere IDE R-Studio. In der Standardeinstellung sieht die Benutzeroberfläche wie folgt aus:\n\nEs gibt vier Hauptbereiche, wobei wir uns hier auf die am häufigsten verwendeten Reiter beschränken:\n\n\n\n\n\n\n\nBereich\nErläuterung\n\n\n\n\nSkript/Quarto-Dokument\nHier befindet sich euer Code\n\n\nR-Console\nHier befindet sich die R-Konsole. Jeder Code, den ihr oben ausführt, erscheint auch in der Konsole. Ihr könnt Code auch direkt in der Konsole ausführen\n\n\nEnvironment\nHier befinden sich eure in dieser Session erzeugten R-Objekte, Werte etc.\n\n\nFiles, Plot & Help\nHier befinden sich die Dateien im Ordner eures R-Projektes, erzeugte Plots & aktivierte Packages sowie die Hilfeseite",
    "crumbs": [
      "Home",
      "Program",
      "Basics"
    ]
  },
  {
    "objectID": "basics.html#code-erstellen-ausführen",
    "href": "basics.html#code-erstellen-ausführen",
    "title": "Basics",
    "section": "Code erstellen & ausführen",
    "text": "Code erstellen & ausführen\n\n\n\n\n\n\nAufgabe\n\n\n\nÖffnet das File welches ihr auf der Installationsseite heruntergeladen habt.\n\n\nIn dem File sind nur die Zeilen ohne # ausführbarer Code. Wie hier geschehen, lassen sich mit # Kommentare in den Code hinzufügen, die dem Code Struktur geben oder erklären, was hier passieren soll\n\n\n\n\n\n\nAufgabe\n\n\n\nFührt die Zeile Code aus, in dem ihr euren Cursor in die Zeile stellt und STRG+ENTER bzw. CMD+ENTER drückt\n\n\nÜber das Markieren von mehreren/allen Zeilen werden die entsprechenden Zeilen nacheinander ausgeführt\n\n# R-Version:\nprint(R.version$version.string)\n\n[1] \"R version 4.3.2 (2023-10-31)\"\n\n\nVoilá - wir haben unsere erste Code-Zeile ausgeführt. Sie zeigt uns die installierte R-Version an. Wir wollen aber auch wissen, welche R-Studio Version wir installiert haben. Dazu führen wir die zweite Zeile aus:\n\n# R-Studio Version:   \nprint(rstudioapi::versionInfo()$version)",
    "crumbs": [
      "Home",
      "Program",
      "Basics"
    ]
  },
  {
    "objectID": "basics.html#r-vs.-quarto",
    "href": "basics.html#r-vs.-quarto",
    "title": "Basics",
    "section": "R vs. Quarto",
    "text": "R vs. Quarto\nAlles klar, wir können nun ein .R Skript öffnen und ausführen. Aber was ist denn nun Quarto?\n\n\n\nR vs. Quarto\n\n\nDas standard Dateiformat für R sind .R Dateien, welche sich über R-Studio öffnen lassen uns wie ein Skript ausgeführt werden können, etwa vergleichbar mit .py Dateien. Im Grunde genommen sind es aber nur Textdateien. Zwar können wir über die oben gelernte Tastenkombination einzelne Code-Zeilen ausführen, was die Bedienung in R-Studio einfacher macht, dennoch haben Quarto-Dateien (Dateiendung .qmd) einige Vorteile:\n\nSie ermöglichen den Export/ das Rendern in andere Dateiformate (html, pdf, docx, pptx,…)\nSie ermöglichen neben Code-Abschnitten auch Text-Abschnitte, die formatiert werden können (wie LaTeX) und die übersichtlicher sind\nSie ermöglichen einzustellen, ob nur Code, nur Output, oder Code+Output angezeigt werden soll, somit können Auswertung & Bericht im gleichen Dokument geschehen\nSie bieten eine Vielzahl an Möglichkeiten (diese ganze Website ist über Quarto geschrieben!)\n\n\nQuarto & Markdown\nWenn ihr in Quarto-Dokumenten Text hinzufügt, ist der Text mithilfe von Markdown formatiert. Markdown ist eine leichte Auszeichnungssprache, die entwickelt wurde, um mit möglichst einfacher Textformatierung lesbare und ästhetische Dokumente zu erstellen. Die Kernidee dabei ist, dass die Syntax einfach zu lesen und zu schreiben sein soll, sowohl in ihrer Rohform als auch nach der Umwandlung in reichhaltigere Formate wie HTML.\n\n\n\n\n\n\nFrage\n\n\n\nMoment mal - noch eine Syntax?\n\n\nGenau richtig, allerdings sollte uns das an der Stelle nicht weiter interessieren, mit Markdown befassen wir uns im Kapitel Communicate genauer. Vorab: R-Studio hat zwei Modi zur Bearbeitung von Dokumenten - Source und Visual:\n\n\n\n\n\n\n\nAufgabe\n\n\n\nSwitcht mal zwischen den Modi und beschreibt, was die Unterschiede sind.\n\n\nAlles klar! Für die meisten Fälle reicht uns der Visual Mode, welcher durch R-Studios Funktionen sehr gut ergänzt wird.\nSchauen wir uns also nun Quarto und die darin vorkommende R-Syntax an.",
    "crumbs": [
      "Home",
      "Program",
      "Basics"
    ]
  },
  {
    "objectID": "basics.html#quarto",
    "href": "basics.html#quarto",
    "title": "Basics",
    "section": "Quarto",
    "text": "Quarto\nUm das Ganze direkt zu testen, sollt ihr nun ein neues Quarto Dokument erstellen. Navigiert dazu zu File/New File/Quarto Document:\n\n\n\nEin neues Quarto Dokument über den Datei Reiter erstellen\n\n\nNun sollte sich ein Fenster öffnen, in dem ihr den Titel eures Dokuments & den Autor einstellen, sowie weitere Einstellungen vornehmen könnt. Wie in folgender Grafik zu sehen, gibt es drei Render-Möglichkeiten:\n\n\n\nEin neues Quarto Dokument erstellen\n\n\n\nRender Optionen\n\n\n\n\n\n\nOption\nSinn\n\n\n\n\nHTML\nR-Studio rendert eine .html Datei, welche ihr mit jedem gewöhnlichen Browser öffnen könnt. Vorteil: Kompatibilität. Nachteil: Manchmal möchte man eine .pdf haben.\n\n\nPDF\nR-Studio rendert mit einer LaTeX Engine eine .pdf Datei. Vorteil: Ihr habt eine .pdf, Nachteil: Das Installieren der Engine sowie das Rendern kann fehleranfällig sein.\n\n\nWord\nR-Studio rendert eine Word-Datei (.docx). Vorteil: Ihr könnt in der Datei herumschreiben, Nachteil: Formatierung etc.\n\n\n\nWir entscheiden uns im Ramen dieses Seminars immer für HTML, da dies das versatilste Format ist. Solltet ihr nun also ein Dokument erstellt haben, sollte es bereits mit Beispielcontent gefüllt sein, und etwas so aussehen:\n\n\n\nR-Studio fügt automatisch Beispielcontent hinzu\n\n\nEs befinden sich also bereits Text, Überschriften & Code in sogenannten Code-Chunks im Dokument. Löscht diesen nun aber heraus, damit wir einen sauberen Start haben.",
    "crumbs": [
      "Home",
      "Program",
      "Basics"
    ]
  },
  {
    "objectID": "basics.html#chunks-hinzufügen",
    "href": "basics.html#chunks-hinzufügen",
    "title": "Basics",
    "section": "Chunks hinzufügen",
    "text": "Chunks hinzufügen\nMit der Tastenkombination COMMAND + OPTION + I bzw. STRG + ALT + I oder folgendem Button:\n\n\n\nNeuer Chunk per GUI\n\n\nkönnt ihr neue Chunks hinzufügen.\n\n\n\n\n\n\nAufgabe\n\n\n\nFügt nun einen Chunk hinzu, und schreibt dort 4+5 hinein:\n\n\n\n4+5\n\n[1] 9\n\n\n\n\n\n\n\n\nAufgabe\n\n\n\nFügt nun die gleiche Rechenaufgabe hinzu, ohne einen neuen Chunk zu erzeugen:\n\n\n4+5\n\n\n\n\n\n\nFrage\n\n\n\nWas fällt auf?\n\n\nWenn ihr nun rendert seht ihr, dass im ersten Fall der Code zu sehen ist, sowie das Ergebnis der Berechnung. Im zweiten Fall dagegen sehen wir nur die Rechnung, sie wurde aber nicht von R evaluiert. Das liegt daran, dass dies nur Markdown-Text war, also kein Code. Code wird also bis auf weiteres nur innerhalb von Chunks ausgeführt.\nNicht immer (eigentlich sehr selten) wollen wir erst rendern, um das Ergebnis unserer Berechnungen zu sehen. Äquivalent zu R-Skripten können wir Code direkt in R-Studio ausführen. Auch dazu können wir unsere gewohnte Tastenkombination STRG + ENTER oder COMMAND + ENTER benutzen, oder oben aus den Play-Button im Chunk drücken (dann wird der ganze Chunk ausgeführt). Das Ganze sieht dann so aus:\n\n\n\nIn R-Studio ausgeführter Code mit Quarto\n\n\n\nNächste Session\nDas war’s mit dieser Session. In der nächsten Session geht es um Syntax & Datentypen.",
    "crumbs": [
      "Home",
      "Program",
      "Basics"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Startseite",
    "section": "",
    "text": "Willkommen auf unserer seminarbegleitenden Website. Hier findet ihr die Skripte der einzelnen Seminarstunden (als .qmd Dateien), Übungsblätter sowie deren Lösungen, als auch viele weitere hilfreiche Informationen.\n\n\n\nFig 1.: Glückliche Studentin, die gerade R gelernt hat"
  },
  {
    "objectID": "dplyr.html",
    "href": "dplyr.html",
    "title": "Transform: dplyr",
    "section": "",
    "text": "Glücklicher Student, der gerade alle tollen dplyr Funktionen entdeckt",
    "crumbs": [
      "Home",
      "Wrangle",
      "Transform"
    ]
  },
  {
    "objectID": "dplyr.html#einleitung",
    "href": "dplyr.html#einleitung",
    "title": "Transform: dplyr",
    "section": "Einleitung",
    "text": "Einleitung\nWir wissen nun, wie wir mithilfe von Base R einige aufregende Sachen machen können. Während wir unsere verschiedenen Bearbeitungs- und Filterungsschritte in einzelnen Objekten speichern können, so kommt die Base-R Syntax doch etwas umständlich daher. Genau da kommt das dplyr Package aus dem tidyverse ins Spiel.\n\nWerkzeug\nIm Wrangling Teil unserer Pipeline ist R vor allem ein Werkzeug, um mit unseren Daten zu arbeiten und sie in eine Form zu bringen, die uns sinnvolle Rückschlüsse hinsichtlich unserer Fragestellungen erlaubt. dplyr unterstützt hierbei und bietet noch bessere Werkzeuge, die darüber hinaus noch das Arbeiten erleichtern, etwa so wie bei einem qualitativ hochwertigen Werkzeugkoffer.\n\nDaher kann es helfen sich die verschiedenen Funktionen in diesem Teil wirklich als Werkzeuge vorzustellen.\n\n\nPipeline Reihenfolge\nIn einer normalen Analysesession gemäß unserer Pipeline würden wir den Datensatz natürlich erst reinigen, bevor wir etwaige Transformationsschritte mit dplyr vornehmen. Aus didaktischer Perspektive ist es aber nicht sinnvoll, dies auch so herum zu lernen.\n\nDaher fangen wir zunächst an, uns mit dplyr und diversen Techniken zur Transformation von Daten zu beschäftigen. Das sollte dann den Weg ebnen für die nächste Session, dem Reinigen von Daten.",
    "crumbs": [
      "Home",
      "Wrangle",
      "Transform"
    ]
  },
  {
    "objectID": "dplyr.html#dplyr",
    "href": "dplyr.html#dplyr",
    "title": "Transform: dplyr",
    "section": "dplyr",
    "text": "dplyr\nViele verschiedene Objekte im Environment sind nur eine Motivation, dplyr zu benutzen. Auf der tidyverse Website steht folgende Beschreibung:\n\ndplyr is a grammar of data manipulation, providing a consistent set of verbs that help you solve the most common data manipulation challenges\n\nEs gehört es zu den beliebtesten R-Packages, besonders wenn es um Datenanalyse geht. Wie wir in der letzten Session gesehen haben können wir (fast) alle Sachen auch mit Base-R machen - da wir aber glauben, dass es sinnvoll ist, gleich zu Beginn mit den “richtigen” state-of-the-art Packages zu arbeiten, schauen wir uns nun dplyr an.",
    "crumbs": [
      "Home",
      "Wrangle",
      "Transform"
    ]
  },
  {
    "objectID": "dplyr.html#der-pipe-operator",
    "href": "dplyr.html#der-pipe-operator",
    "title": "Transform: dplyr",
    "section": "Der Pipe-Operator %>%",
    "text": "Der Pipe-Operator %&gt;%\nIm Zentrum von dplyr steht der sogenannte Pipe-Operator %&gt;% (ursprünglich aus dem Magrittr Package):\n%&gt;%\nDieser schaut zunächst etwas ungewohnt aus, daran gewöhnt man sich aber schnell. Mithilfe des Pipe Operators können wir verschiedene Befehle aneinanderketten. Das Ganze sieht dann so wie folgt aus:\n\ndataframe %&gt;% \n  do_something(parameters = \"xyz\")\n\nDer Output des jeweiligen Befehls wird sozusagen als Input in die nächste Zeile “gepiped”. Das heißt, wenn der Input ein Dataframe ist, mit dem dann entsprechend Anpassungen vorgenommen werden, so ist der Output für die nächste Zeile wieder ein Dataframe - welches diese dann weiterverarbeitet.\nFür den Operator gibt es natürlich eine Tastenkombination, mithilfe derer wir ihn schnell eingeben können. Diese lautet COMMAND + SHIFT + M für Mac-User, und STRG + SHIFT + M für Windows User.\nWir erinnern uns, dass der obige Befehl üblicherweise so lauten würde:\n\ndo_something(dataframe, parameters = \"xyz\")\n\nHier müssen wir das betreffende Objekt als Parameter übergeben, und anschließend den Output entweder direkt verwerten, oder über &lt;- speichern. Wenn wir nun eine zweite Funktion do_something_else für das gleiche Objekt verwenden wollten, müssten wir dies erneut speichern, oder überspeichern:\n\nresult &lt;- do_something(dataframe, parameters = \"xyz\")\n\ndo_something_else(result, parameters = \"abc\")\n\nMithilfe des Pipe Operators %&gt;% können wir uns das sparen. Wir starten mit dem ursprünglichen Objekt, und geben dann die bearbeiteten Versionen in die jeweilige Zeile weiter:\n\ndataframe %&gt;% \n  do_something(parameters = \"xyz\") %&gt;% \n  do_something_else(parameters = \"abc\")\n\nDer Output von do_something() wird also zum Input von do_something_else() .",
    "crumbs": [
      "Home",
      "Wrangle",
      "Transform"
    ]
  },
  {
    "objectID": "dplyr.html#funktionen",
    "href": "dplyr.html#funktionen",
    "title": "Transform: dplyr",
    "section": "Funktionen",
    "text": "Funktionen\ndplyr ist aber natürlich nicht nur wegen des Pipe-Operators so praktisch. Es bringt auch viele Funktionen mit sich, die bei typischen Datenanalyse-Aufgaben relevant sind. Diese Funktionen ermöglichen ähnliche Dinge wie bspw. das auf der vorigen Seite gezeigte subset(), sind dabei jedoch etwas weniger umständlich und intuitiver. Im Folgenden wollen wir euch die wichtigsten davon vorstellen. Um das gleich anhand eines Datensatzes zu üben, verwenden wir den iris Datensatz aus der letzten Session.\nFalls der Datensatz aus der letzten Session nicht mehr im Environment ist, lesen wir ihn kurzerhand neu ein:\n\niris &lt;- read_csv(\"assets/datasets/iris/iris.csv\")\n\n\nfilter(x)\nAngenommen, wir wollen unseren Datensatz filtern, so wie im vorherigen Kapitel. Mit der filter() Funktion von dyplr können wir, wie der Name schon sagt, Datensätze auf Basis gewisser Kriterien filtern.\n\nThe filter() function is used to subset a data frame, retaining all rows that satisfy your conditions. To be retained, the row must produce a value of TRUE for all conditions. Note that when a condition evaluates to NA the row will be dropped, unlike base subsetting with [. (tidyverse.org)\n\n\nAngenommen also, wir wollen nach der Spezies setosa filtern. Dies geschieht mit dplyr wie folgt:\n\niris %&gt;% \n  filter(species == \"setosa\")\n\n\n  \n\n\n\n\n\n\n\n\n\nFrage\n\n\n\nWie können wir mehrere Bedingungen kombinieren?\n\n\nDas funktioniert wie folgt:\n\niris %&gt;% \n  filter(species == \"setosa\" & sepal_length &lt; 5)\n\n\n  \n\n\n\n\n\n\n\n\n\nFrage\n\n\n\nWas wäre eine alternative Möglichkeit, nach zwei Bedingungen zu filtern, die uns dplyr bietet?\n\n\nDas besondere an dplyr ist natürlich die Möglichkeit zur Verkettung über den Pipe-Operator %&gt;%.\n\niris %&gt;% \n  filter(species == \"setosa\") %&gt;% \n  filter(sepal_length &lt; 5)\n\n\n  \n\n\n\nWie zu sehen ist, sind die resultierenden Dataframes dieselben.\n\n\nselect(x)\nÄhnlich wie in Base-R mit dem $ Operator oder subset(x) können wir mithilfe von select(x) einzelne Spalten auswählen.\n\nSelect (and optionally rename) variables in a data frame, using a concise mini-language that makes it easy to refer to variables based on their name (e.g. a:f selects all columns from a on the left to f on the right) or type (e.g. where(is.numeric) selects all numeric columns). (tidyverse.org)\n\n\nDie Handhabung ist hierbei noch etwas leichter als in Base-R, und die Syntax ist wie folgt:\n\niris %&gt;% \n  select(sepal_length,sepal_width)\n\n\n  \n\n\n\nDamit wählen wir die Spalten sepal_length und sepal_width aus. Wir könnten auch sagen, wir wollen alle Spalten außer species:\n\niris %&gt;% \n  select(-species)\n\n\n  \n\n\n\nWenn wir die Reihenfolge der Spalten (von links nach rechts) kennen, können wir auch alle Spalten (von - bis) auswählen, etwa so:\n\niris %&gt;% \n  select(sepal_length:petal_width)\n\n\n  \n\n\n\nOft kommt es vor, dass wir bspw. durch Limesurvey wissen, dass alle Variablen eines Fragebogens mit SQ.. anfangen. Sollten wir diese alle schnell auswählen wollen (bspw. zum Berechnen von Summenscores), kann uns dplyr auch dabei helfen, mithilfe von starts_with().\n\niris %&gt;% \n  select(starts_with(\"se\"))\n\n\n  \n\n\n\nWir sehen, dass nun nur die Spalten ausgewählt wurden, die mit se anfangen. Nach dem gleichen Prinzip gibt es auch die ends_with(x), matches(x) oder contains(x) Funktionen.\n\n\nmutate(x)\nApropos Summenscores - mutate() klingt gruseliger, als es ist. Mit mutate() können wir neue Variablen kreieren, oder bestehende modifizieren.\n\nmutate() creates new columns that are functions of existing variables. It can also modify (if the name is the same as an existing column) and delete columns (by setting their value to NULL). (tidyverse.org)\n\n\nAngenommen, die Blätter der Iris-Blume wären rechteckig, und wir könnten die Fläche petal_square einfach in Quadratzentimeter berechnen:\n\niris %&gt;% \n  mutate(petal_square = petal_length*petal_width)\n\n\n  \n\n\n\nWir sehen, die Syntax ist mutate(new_variable = operation). Wir können auch mehrere Variablen in einem Zug erzeugen:\n\niris %&gt;% \n  mutate(petal_square = petal_length*petal_width,\n         sepal_square = sepal_length*sepal_width)\n\n\n  \n\n\n\n\n\n\n\n\n\nFrage\n\n\n\nWas passiert, wenn der neue Variablenname in einem mutate(x) call der gleiche wie der alte ist?\n\n\nWenn der Name von new_variable gleich wie der alte ist, überschreiben wir die Spalte:\n\niris %&gt;% \n  mutate(sepal_length = sepal_length/10)\n\n\n  \n\n\n\nVoila! Allein mit der mutate(x) Funktion haben wir bereits ein so mächtiges Tool in der Hand, dass Tools wie SPSS die blanke Panik bekommen.\n\n\narrange(x)\nManchmal ist es sinnvoll, einen Datensatz zu sortieren. Im dplyr Jargon heißt das arrange(x):\n\narrange() orders the rows of a data frame by the values of selected columns. Unlike other dplyr verbs, arrange() largely ignores grouping; you need to explicitly mention grouping variables (or use  .by_group = TRUE) in order to group by them, and functions of variables are evaluated once per data frame, not once per group. (tidyverse.org)\n\n\n\niris %&gt;% \n  arrange(sepal_length)\n\n\n  \n\n\n\nWir sehen, die Spalte sepal_length wird in aufsteigender Reihenfole sortiert. Wollen wir es in absteigender Reihenfolge haben, müssen wir die Funktion desc() dazunehmen:\n\niris %&gt;% \n  arrange(desc(sepal_length))\n\n\n  \n\n\n\nSuper! Nun können wir uns den Datensatz also in sortierter Form anschauen.\n\n\ngroup_by(x)\nEine große Besonderheit von dplyr ist die Möglichkeit, Berechnungen schnell und einfach auf Gruppenebene zu machen. Dazu gibt es die group_by(x) Funktion.\n\nMost data operations are done on groups defined by variables. group_by() takes an existing tbl and converts it into a grouped tbl where operations are performed “by group”. ungroup() removes grouping. (tidyverse.org)\n\n\nUm wirklich zu verstehen, was group_by(x) tut, müssen wir das Ganze aber etwas genauer aufschlüsseln. Die generelle Syntax ist die folgende:\n\nobject %&gt;% \n  group_by(variable) %&gt;% \n  do_something()\n\nWir übergeben der group_by() Funktion als Parameter eine Variable, nach der diese den Datensatz gruppieren soll. Hier ist es wichtig zu verstehen, dass diese Gruppierung für alle nachfolgenden Schritte gilt, selbst aber keinen Effekt hat. Was heißt das genau? Betrachten wir folgenden Code:\n\niris %&gt;% \n  group_by(species)\n\n\n  \n\n\n\nAußer der Info Groups: species [3] hat sich nichts verändert. Nehmen wir allerdings in der nächsten Zeile eine Funktion hinzu (bspw. mutate()), so verändert das, wie diese sich verhält. Normalerweise würde folgender Code die Variable sepal_length_mean erzeugen, welche den Mittelwert von sepal_length enthält:\n\niris %&gt;% \n  mutate(sepal_length_mean = mean(sepal_length))\n\n\n  \n\n\n\nDa dieser natürlich für den gesamten Datensatz (also über alle Zeilen hinweg) berechnet wird, beträgt er für jede Messung 5.84. Wenn wir vorher allerdings group_by() eingeben, sieht das anders aus:\n\niris %&gt;% \n  group_by(species) %&gt;% \n  mutate(sepal_length_mean = mean(sepal_length))\n\n\n  \n\n\n\n\n\n\n\n\n\nFrage\n\n\n\nWas ist anders?\n\n\nWir sehen, dass mutate() den Wert pro Gruppe berechnet hat. Dies kann sehr hilfreich für verschiedenste Anwendungen sein. Warum genau und wie wir nach mehreren Variablen gruppieren, dazu gleich mehr.\n\n\nsummarise(x)\nVielleicht wirkt es etwas merkwürdig, dass wir oben die Variable sepal_length_mean für jeden Datenpunkt einzeln berechnet haben, obwohl diese natürlich immer gleich ist. Meistens wollen wir solche Werte für die gesamte Stichprobe haben. Dabei hilft uns summarise(x).\n\nsummarise() creates a new data frame. It returns one row for each combination of grouping variables; if there are no grouping variables, the output will have a single row summarising all observations in the input. It will contain one column for each grouping variable and one column for each of the summary statistics that you have specified. summarise() and summarize() are synonyms (tidyverse.org)\n\n\nDie Syntax ist ähnlich wie bei den anderen Funktionen von dplyr und sieht wie folgt aus:\n\niris %&gt;% \n  summarise(sepal_length_mean = mean(sepal_length))\n\n\n  \n\n\n\nWir sehen, dass die Syntax dieselbe wie die von mutate(x) ist, nur dass der Output eben nur eine Zahl (bzw. eine Zeile ist) - eben die Summary unseres Datensatzes unter den gegebenen Bedingungen. Besonders hilfreich ist summarise(x) in Kombination mit group_by(x):\n\niris %&gt;% \n  group_by(species) %&gt;% \n  summarise(sepal_length_mean = mean(sepal_length))\n\n\n  \n\n\n\nSo können wir mit drei Zeilen die Mittelwerte für die jeweilige Gruppe sehen.\n\n\n\n\n\n\nAufgabe\n\n\n\nBerechnet nicht nur den mean, sondern gleichzeitig auch die Standardabweichung\n\n\nWir können natürlich auch gleich die Standardabweichung mitberechnen:\n\niris %&gt;% \n  group_by(species) %&gt;% \n  summarise(sepal_length_mean = mean(sepal_length),\n            sepal_length_sd = sd(sepal_length))\n\n\n  \n\n\n\nEin geschickter Einsatz von group_by(x) und summarise(x) kann extrem hilfreich sein, und in Sekunden einen Überblick über die Daten gewährleisten.\n\n\n\n\n\n\nFür Expert:innen\n\n\n\nOben wurde kurz erwähnt, dass wir auch nach mehreren Variablen gruppieren können. Angenommen, es gäbe noch zusätzlich die Variable color für die Blütenfarbe in den Ausprägungen purple, blue und white für jeden Datenpunkt, d.h., jede Spezies kann auch eine dieser verschiedenen Farben haben.\n\n\nCode\nset.seed(44)\niris %&gt;% \n  mutate(color = as.factor(sample(rep(c(\"purple\",\"blue\",\"white\"),50)))) -&gt; iris\n\n\nWenn wir nun nach den Variablen species und color gruppieren wollen, müssen wir diese beiden Variablen als Parameter übergeben:\n\niris %&gt;% \n  group_by(species, color) %&gt;% \n  summarise(sepal_length_mean = mean(sepal_length),\n            sepal_length_sd = sd(sepal_length))\n\n\n  \n\n\n\nNun sehen wir die Mittelwerte für die verschiedenen Kombinationen der Variablen (also bspw. setosa & blue). Hier kann es hilfreich sein, die Anzahl an Zeilen pro Gruppe zu bekommen. Hierfür eignet sich die n() Funktion:\n\niris %&gt;% \n  group_by(species, color) %&gt;% \n  summarise(sepal_length_mean = mean(sepal_length),\n            sepal_length_sd = sd(sepal_length),\n            n = n())\n\n\n  \n\n\n\nSuper! Nun haben wir die Mittelwerte für jede Kombination aus species und color, sowie die Anzahl n der jeweiligen Zeilen bzw. Messungen.",
    "crumbs": [
      "Home",
      "Wrangle",
      "Transform"
    ]
  },
  {
    "objectID": "dplyr.html#temporär-oder-speichern",
    "href": "dplyr.html#temporär-oder-speichern",
    "title": "Transform: dplyr",
    "section": "Temporär oder Speichern?",
    "text": "Temporär oder Speichern?\nSicherlich ist aufgefallen, dass alle unsere gezeigten “Pipes” nur zu einem Output im Chunk geführt haben, wir diese allerdings nicht in unserem Environment gespeichert haben. Natürlich können wir dies tun, indem wir unsere Pipe einem Objekt über den Zuweisungsoperator &lt;- zuweisen:\n\nobject &lt;- object %&gt;% \n            select(variableA,variableB,variableC) %&gt;% \n            mutate(variableA*100)\n\nLetzendlich ist uns selbst überlassen, wie wir mit den Daten umgehen. Manchmal wird es Fälle geben, in denen Speichern Sinn ergibt (bspw. wenn sonst immer wieder die gleiche Transformation anstünde), oft wird es aber auch ausreichend sein, die Daten einfach im Output des jeweiligen Chunks anzuschauen, bzw. etwaige Transformationen unmittelbar vor weiteren Schritten zu machen (bspw. beim Erzeugen von Plots). Unsere Empfehlung lautet: Nutzt den großen Vorteil der Reproduzierbarkeit von R und speichert am besten so wenig wie möglich zwischen und nutzt lieber einmal mehr als weniger Pipes.\n\n\n\n\n\n\nAchtung\n\n\n\nWenn wir mit summarise(x) einen Output bekommen, so ist dieser natürlich nicht mehr das “ursprüngliche” Dataframe, sondern ein neues. Demnach beziehen sich neue Operationen, die wir einem summarise(x) call anschließen, auch auf dieses neue Dataframe. Daher ist summarise(x) häufig das Ende einer Pipe-Operation, und wir betrachten den Output im Chunk.",
    "crumbs": [
      "Home",
      "Wrangle",
      "Transform"
    ]
  },
  {
    "objectID": "dplyr.html#fazit",
    "href": "dplyr.html#fazit",
    "title": "Transform: dplyr",
    "section": "Fazit",
    "text": "Fazit\ndplyr bringt eine einfach verständliche Grammatik mit sich, die uns hilft, uns auf das Wesentliche zu konzentrieren: Die Daten.\nIn der nächsten Session werden wir sehen, wie wir mit Daten umgehen können, die noch etwas unsauber sind, und zwar mit dem Package tidyR.",
    "crumbs": [
      "Home",
      "Wrangle",
      "Transform"
    ]
  },
  {
    "objectID": "syntax_datatypes.html",
    "href": "syntax_datatypes.html",
    "title": "Syntax & Datentypen",
    "section": "",
    "text": "Studentin, die endlich versteht, wie die R-Syntax funktioniert",
    "crumbs": [
      "Home",
      "Program",
      "Syntax & Datentypen"
    ]
  },
  {
    "objectID": "syntax_datatypes.html#einleitung",
    "href": "syntax_datatypes.html#einleitung",
    "title": "Syntax & Datentypen",
    "section": "Einleitung",
    "text": "Einleitung\nDie Syntax von R ist relativ simpel. Als interpretierte Sprache (im Gegensatz zu kompilierten Sprachen wie C oder Java) müsst ihr nicht erst etwaige Klassen oder Objekte erzeugen und den Code kompilieren, um loszulegen, sondern könnt einzelne Zeilen direkt ausführen. Da es eine Statistiksoftware ist, wurden R quasi die Möglichkeiten statistischen Fragestellungen nachzugehen in die Wiege gelegt. Dazu gehört also auch die schnelle Berechnung einer mathematischen Aufgabe, wie wir bspw. letzte Woche gesehen haben:\n\n4+5\n\n[1] 9\n\n\nWie wir sehen reicht eine Zeile in einem R Code-Chunk aus, um die Aufgabe zu berechnen. Wichtig ist hier zu verstehen, dass bei Quarto-Dokumenten ausführbarer Code also immer in Code-Chunks ist, während Text (bspw. zur Beschreibung) zwischen den Chunks ist.\n\nKommentare\nNatürlich können wir aber auch in den Chunks selbst Text schreiben. Damit dieser nicht von R mit ausführbarem Code verwechselt wird, müssen wir diesen aber mit # als Kommentar kennzeichnen, etwa so:\n\n# Rechnung:\n4+5\n\n[1] 9\n\n\nSo weiß der R Interpreter, dass es sich um ein Kommentar handelt und beachtet diese Zeile nicht. Das besondere an R ist, dass R natürlich noch viel mehr bietet, und viele Vorteile, die andere Programmiersprachen wie bspw. Python mitbringen, ebenfalls mitbringt (bspw. Funktionen, Schleifen etc.). Es ist also ein ganz schön mächtiges Tool.\n\n\nUnterschiede zu anderen Programmiersprachen\nR unterscheidet bei Zahlen-Datentypen nicht zwischen Integer (42) und Double (42.0) (wie etwa Java), aber dazu gleich mehr. Strings heißen character, und außerdem verwendet es keine Pointer (wie bspw. Python). Tendenziell könnt ihr euch merken: Viele Angelegenheiten, bei denen andere Sprachen empfindlich sind, sind in R simpler aufgebaut - um einer schnellen statistischen Auswertung keine Steine in den Weg zu legen!\n\n\n(Rechen)operatoren\nNatürlich gibt es noch weitere Rechenoperatoren als +. Viele davon solltet ihr schon aus anderen Programmiersprachen gewohnt sein. Der Einfachheit halber berichten wir hier sowohl mathematische als auch andere Operatoren:\n\n\n\n\n\n\nAufgabe\n\n\n\nErstellt einen neuen Code Chunk und macht für jeden Operator ein Beispiel. Wählt bei logischen Operatoren die Zahlen oder Variablen so, dass TRUE zurückgegeben wird.\n\n\n\n\n\n\n\n\n\n\n(Rechen)operator\nFunktion\nBeispiel\n\n\n\n\n+\nAddition\n37 + 5\n\n\n-\nSubtraktion\n48 - 6\n\n\n*\nMultiplikation\n7 * 6\n\n\n/\nDivision\n546 / 13\n\n\n^\nExponent\n6.480741 ^ 2\n\n\nsqrt(x)\nWurzel\nsqrt(1764)\n\n\n&gt;\nGrößer\n40+3 &gt; 42\n\n\n&lt;\nKleiner\n41 &lt; 546/13\n\n\n&gt;=\nGrößer gleich\n42 &gt;= 21*2\n\n\n&lt;=\nKleiner gleich\n7*6 &lt;= sqrt(1764)\n\n\n==\nLogischer Vergleich (gleich)\n42 == sqrt(1764)\n\n\n!=\nLogischer Vergleich (ungleich)\n\"R\" != \"R-Studio\"\n\n\n&\nLogisches Und\nTrue & True\n\n\n|\nLogisches Oder\nTrue & True\n\n\nxor\nLogisches exklusives Oder\nFalse & True\n\n\n!\nLogisches Nicht\n! False\n\n\n\n\n\n\n\n\n\nFrage\n\n\n\nWas fällt auf?\n\n\nJe nach Operator ist die Syntax etwas anders (bspw. bei sqrt(x)). Warum das so ist, klären wir in dieser Session. Außerdem ist vielleicht aufgefallen, dass wir mit verschiedenen Arten von Daten arbeiten können, wie bspw. Strings, logischen Werten oder (Komma)zahlen. Das leitet zu Datentypen über.\n\n\nDatentypen\nWie in anderen Programmiersprachen gibt es verschiedene Datentypen in R, hier eine Aufzählung der häufigsten:\n\n\n\n\n\n\n\n\nDatentyp\nBeschreibung\nBeispiel\n\n\n\n\nnumeric\nZahlen (integer, double)\n42, 42.0, 42.001\n\n\ncharacter\nZeichen/Buchstabenfolgen, Strings\n\"42\", \"Zweiundvierzig\"\n\n\nlogical\nLogische Werte\nTRUE, FALSE\n\n\nfactor\nz.B. Faktorstufen eines Faktors\n\"blonde\", \"red-haired\", \"brunette\"\n\n\nvector\nEindimensionale Reihe von Elementen\nc(1, 2, 3, 4\n\n\nmatrix\nEin- bis zweidimensionale Reihe von Elementen\nmatrix(c(1,2,3, 11,12,13), nrow = 2, ncol = 3, byrow = TRUE)\n\n\nliste\nKann verschiedene Datentypen & Strukturen enthalten\nlist(c(1,2,3),c(4,5,6))\n\n\nDataframe/Tibble\nHäufigstes Format zur Darstellung von Datensätzen mit unterschiedlichen Datentypen pro Spalte\ndata.frame(x=c(1,2,3),y=c(4,5,6))\n\n\n\n\n\n\n\n\n\nFrage\n\n\n\nWie können wir uns factor nun konkret vorstellen, und warum bestehen list und data.frame aus mehreren Objekten? Und warum sind manche Dinge in Klammern?\n\n\n\n\nBasisfunktionen\nVektoren oder Dataframes werden über sogenannte Funktionen erzeugt, erkennbar an Klammern. Eine Funktion ist in R immer gleich aufgebaut:\nfunction(value, parameter)\nEin Beispiel wäre die obige Funktion zum Berechnen der Quadratwurzel:\n\nsqrt(1764)\n\n[1] 42\n\n\nfunction ist hier also der Name der Funktion (sqrt) und value ist der Wert, den wir der Funktion übergeben (1764). In diesem Fall brauchen wir keine weiteren Parameter übergeben, daher belassen wir es bei der Zahl. Wenn wir bspw. eine Zahl runden möchten, können wir die round(x) Funktion nehmen:\n\nround(3.14159, digits = 2)\n\n[1] 3.14\n\n\nWie wir sehen, haben wir hier 3.14159 als value übergeben und 2 als parameter (in dem Fall digit also die Anzahl an Nachkommastellen).\nFunktionen kommen überall in R in verschiedensten Formen und Farben vor, und die Grundsyntax ist wie oben beschrieben.\n\n\nVariablen & Objekte\nWenn wir nun wirklich mit Daten arbeiten wollen und diese nicht nur flüchtig betrachten, müssen wir Werte in Variablen speichern. Dabei ist in R vor allem der Zuweisungsoperator &lt;- wichtig. Mit diesem Operator können wir Variablen bzw. Objekte erzeugen und diese in unserem Environment speichern. Das funktioniert immer nach dem Prinzip:\nvariable &lt;- value\nWobei dann unter variable die value gespeichert wird. Hier ein Beispiel:\n\nnumber &lt;- 1764\n\nsqrt(number)\n\n[1] 42\n\n\nnumber taucht nun also in meinem Environment unter Values auf:\n\nUnd ich kann während der gesamten R-Session darauf zugreifen:\n\nnumber\n\n[1] 1764\n\n\n\n\n\n\n\n\nAchtung\n\n\n\nEs ist wichtig, dass ihr immer &lt;- als Zuweisungsoperator benutzt. Das Gleichzeichen = (wie bspw. in Python) funktioniert nicht verlässlich.\n\n\nMit der Funktion typeof(x) bzw. den Funktionen is.factor(x), is.numeric(x), oder is.character(x) können wir bspw. überprüfen, welchen Typ eine Variable hat bzw. ob der Typ einer Variable bspw. numeric ist. Mit as.numeric(x) können wir den Typ einer Variable verändern. Dann würde ich etwa so vorgehen:\n\nnumber &lt;- as.character(number)\n\nnumber\n\n[1] \"1764\"\n\n\nWie wir sehen, wurde der Typ der Variablen number nun verändert.\n\n\n\n\n\n\nAchtung\n\n\n\nDer Operator &lt;- kann natürlich nicht nur zum erstmaligen Zuweisen von Werten/Variablen verwendet werden, sondern auch zum Überschreiben (wie im Beispiel oben). Auch hier gilt das Prinzip: old_value &lt;- new_value.\n\n\nNeben einzelnen Werten gibt es aber auch Objekte, mit denen wir in R arbeiten können und die wir in unser Environment speichern können. Dazu gehören etwa die oben genannten list, data.frame, vector oder matrix.\n\nVector\nWenn wir mehrere Werte als Vektoren darstellen bzw. abspeichern wollen, benutzen wir die c(x) Funktion. Ich kann also mehrere Werte wie 42,43,44,45 wie folgt als Vektor erzeugen:\n\n# Basismethode\nc(42,43,44,45)\n\n[1] 42 43 44 45\n\n# Alternativmethode\nc(42:45)\n\n[1] 42 43 44 45\n\n\nZum Abspeichern benutzen wir nun wieder den &lt;- Operator:\n\nvec &lt;- c(42:45)\n\nvec\n\n[1] 42 43 44 45\n\n\nWir können natürlich auch andere Datentypen als numeric abspeichern, wie bspw. character:\n\nvec1 &lt;- c(\"Zweiundvierzig\",\"Dreiundvierzig\",\"Vierundvierzig\",\"Fünfundvierzig\")\n\nvec1\n\n[1] \"Zweiundvierzig\" \"Dreiundvierzig\" \"Vierundvierzig\" \"Fünfundvierzig\"\n\n\n\n\nDataframe\nDie meisten Daten mit denen wir hantieren (vor allem im lernpsychologischen Bereich) sind aber komplexer als eindimensionale Vektoren, und sind meistens in tabellarischer Form vorhanden. Dies wird über den data.frame Datentypen abgebildet. Diesen können wir manuell erzeugen, meistens tun wir das aber nicht (sondern importieren ihn etc.). Intern handelt es sich aber trotzdem fast immer um Dataframes. Daher ist deren Verständnis wichtig.\n\n\n\n\n\n\nInfo\n\n\n\nEs kann helfen sich vorzustellen, dass Dataframes im Prinzip aus Vektoren bestehen, die aneinander gefügt werden - vertikal oder horizontal, je nach Sichtweise.\n\n\nWir können also ein Dataframe manuell erstellen, entweder mit spezifischen Werten oder mit bereits vorhandenen Vektoren. Wir wollen nun die obige Aussage testen und ein Dataframe aus den beiden Vektoren vec und vec1 erstellen. Dazu benutzen wir die data.frame(x) Funktion, und weisen von uns definierten Spalten die oberen zwei Vektoren vec und vec1 zu.\n\ndf &lt;- data.frame(num=vec,\n                 char=vec1)\n\ndf\n\n  num           char\n1  42 Zweiundvierzig\n2  43 Dreiundvierzig\n3  44 Vierundvierzig\n4  45 Fünfundvierzig\n\n\nDie expliziten Namen haben wir entschieden, um die Zuordnung zu erleichtern. Unser Dataframe taucht nun in unserem Environment unter Data anstatt Values auf, da es sich um ein Objekt handelt:\n\nDurch ein Klicken auf das Dataframe (oder die Funktion View(df))können wir es uns im Datenviewer anschauen:\n\nDas kann sehr hilfreich sein, um einen Überblick über die Daten zu bekommen.\n\n\nList\nAngenommen, wir haben zwei Dataframes, und wollen diese zusammen abspeichern, so können wir diese in einer Liste abspeichern:\n\ndf1 &lt;- data.frame(x=c(7,8,9),y=c(10,11,12))\n\nlst &lt;- list(df,df1)\n\nVoilá! Nun haben wir beide Dataframes in einem Listenobjekt abgespeichert.\nEs gibt noch weitere Datentypen wie matrix, welche wir im Rahmen dieses Seminars aber nicht weiter besprechen werden.\n\n\n\nElemente & Objekte auswählen\nWir kennen nun die Basis-R Syntax, wissen wie wir Werte und Objekte erzeugen und in unserem Environment speichern und wissen, wie wir Dataframes erzeugen. Manchmal möchten wir auf einzelne Were (bspw. Spalten) zugreifen. Zu wissen, wie das mit der Basis-R Syntax geht ist sehr hilfreich, daher schauen wir uns das nun an.\n\nVektoren\nWir haben oben den Vektor vec erstellt, welcher vier Zahlen enthält:\n\nvec\n\n[1] 42 43 44 45\n\n\nWenn wir nun auf das zweite Element dieses Vektors zugreifen wollen, können wir dies wie folgt tun:\n\nvec[2]\n\n[1] 43\n\n\nAufmerksame Programmierer:innen stutzen hier vielleicht kurz, weil wir hier eine 2 anstatt einer 1 verwenden. Der Hintergrund: In R startet der Index bei 1 anstatt bei 0, wie bspw. bei Python oder anderen Programmiersprachen.\nSoweit, so gut. Doch was ist, wenn ich an spezifische Daten in meinem Dataframe df kommen möchte?\n\n\nDataframes\nUnser erzeugtes Dataframe df hat zwei Spalten und vier Zeilen. Angenommen, wir wollen die zweite Spalte haben, welche ursprünglich unseren Vektor vec1 enthält, dann gehen wir wie folgt vor:\n\ndf[,2]\n\n[1] \"Zweiundvierzig\" \"Dreiundvierzig\" \"Vierundvierzig\" \"Fünfundvierzig\"\n\n\n\n\n\n\n\n\nFrage\n\n\n\nWofür ist das Komma, und was könnte vor das Komma kommen?\n\n\nDie Auswahl funktioniert nach folgender Syntax:\ndataframe[row, column]\nWenn wir also die gesamte zweite Spalte haben möchten, gehen wir wie oben vor. Wenn wir bspw. nur die zweite Zeile haben wollen, gehen wir so vor:\n\ndf[2,]\n\n  num           char\n2  43 Dreiundvierzig\n\n\nNach gleichem Prinzip können wir uns auch einzelne Werte anschauen:\n\ndf[4,2]\n\n[1] \"Fünfundvierzig\"\n\n\nWir können auch Spaltennamen verwenden:\n\ndf[,\"char\"]\n\n[1] \"Zweiundvierzig\" \"Dreiundvierzig\" \"Vierundvierzig\" \"Fünfundvierzig\"\n\n\nSofern Spaltennamen vorhanden sind, können wir auch den $ Operator nehmen, um auf die Spalte zuzugreifen:\n\ndf$char\n\n[1] \"Zweiundvierzig\" \"Dreiundvierzig\" \"Vierundvierzig\" \"Fünfundvierzig\"\n\n\nDer obige Code tut dasselbe wie df[,\"char\"]. Die Voraussetzung ist, dass das Objekt ein Dataframe ist.\n\n\n\n\n\n\nFrage\n\n\n\nWofür ist das wichtig?\n\n\nWir könnten nun fragen, wie häufig wir in der tatsächlichen Praxis auf einzelne Werte über die oben gezeigte Syntax zugreifen wollen - zurecht! Wahrscheinlich eher selten - trotzdem ist es wichtig, das Prinzip von Dataframes und Vektoren verstanden zu haben, da es uns später bei einer Vielzahl von Anwendungen helfen kann, und die seltsam anmutende Syntax und Begriffe wie Vektoren und Dataframes ein Stück weit entzaubert.\n\n\n\nNächste Session\nDas war’s mit dieser Session. In der nächsten Session geht es um Packages & Help.",
    "crumbs": [
      "Home",
      "Program",
      "Syntax & Datentypen"
    ]
  },
  {
    "objectID": "tidy.html",
    "href": "tidy.html",
    "title": "Tidy: tidyr",
    "section": "",
    "text": "Bemühte Studentin, die gerade Daten reinigt (fast)",
    "crumbs": [
      "Home",
      "Wrangle",
      "Tidy"
    ]
  },
  {
    "objectID": "tidy.html#einleitung",
    "href": "tidy.html#einleitung",
    "title": "Tidy: tidyr",
    "section": "Einleitung",
    "text": "Einleitung\nDaten sind leider nicht immer so schön sauber wie in vielen Beispieldatensätzen. Oft passen die Variablennamen nicht oder sind irreführend, Spalten sind falsch kodiert oder enthalten den falschen Datentyp, oder es liegen noch grundlegendere Probleme vor.\nAls echte R-Data Scientists müssen wir damit natürlich umgehen können. Auch hier gibt es sowohl in Base-R, vor allem aber auch durch die Packages tidyr und stringr aus dem tidyverse Package eine Vielzahl an Möglichkeiten und Funktionen, mit “unsauberen” Daten umzugehen. Das Ziel dieser Session soll sein, einige Funktionen dieser Packages kennenzulernen, um eine Grundlage für selbstständige Recherche in der Zukunft zu schaffen. Hier aber noch ein paar Infos zu den Packages:\n\ntidyr\nThe goal of tidyr is to help you create tidy data. Tidy data is data where:\n\nEach variable is a column; each column is a variable.\nEach observation is a row; each row is an observation.\nEach value is a cell; each cell is a single value.\n\n\n\nstringr\nStrings are not glamorous, high-profile components of R, but they do play a big role in many data cleaning and preparation tasks. The stringr package provides a cohesive set of functions designed to make working with strings as easy as possible\n\nBeide Packages sind über library(tidyverse) bereits in eurem Environment aktiviert und müssen nicht nochmal gesondert aktiviert werden.",
    "crumbs": [
      "Home",
      "Wrangle",
      "Tidy"
    ]
  },
  {
    "objectID": "tidy.html#tidying",
    "href": "tidy.html#tidying",
    "title": "Tidy: tidyr",
    "section": "Tidying",
    "text": "Tidying\n\nEinlesen & verstehen\nZu Übungszwecken haben wir einen “messy” Datensatz vorbereitet, speziell den iris Datensatz, welchen wir bereits kennen. Ladet diesen herunter und anschließend in euer Environment:\n\n\n\n\n\n\n\nHinweis\n\n\n\nUm die .csv Datei herunterzuladen, klickt ihr mit der rechten Maustaste auf den Link und klickt auf Verknüpfte Datei speichern unter\n\n\n\niris_messy &lt;- read.csv(\"assets/datasets/iris/iris_messy.csv\")\n\n\n\n\n\n\n\nHinweis\n\n\n\nBenutzt hier bitte die Base-R read.csv(x) Funktion, anstatt der dplyr read_csv(x) Funktion, da wir den Datensatz so wie er ist importieren wollen.\n\n\nUnd werfen einen schnellen Blick auf die Daten:\n\niris_messy %&gt;% \n  head()\n\n\n  \n\n\n\n\n\n\n\n\n\nTipp\n\n\n\nWir benutzen hier die head(x) Funktion, um nicht das ganze Dataframe anzuzeigen.\n\n\nEin genauerer Blick zeigt, dass hier einige Probleme vorliegen.\n\n\nProbleme\n\n\n\n\n\n\nFrage\n\n\n\nWas für Probleme gibt es?\n\n\nRichtig, es gibt insgesamt 5 Probleme:\n\nSpalte X mit Zeilennummern brauchen wir nicht\nVariablennamen sind großgeschrieben\nVARIABLE.(..) vor den Blattgrößen brauchen wir nicht\nDie Variablen sind nicht numeric, und wir haben das falsche Dezimaltrennzeichen (, anstatt .)\nDie spalte type hilft uns so nicht weiter, da wir einzelne Informationen zu species und color brauchen\n\nAber keine Sorge! Wir nehmen uns all dieser Probleme in dieser Session an.",
    "crumbs": [
      "Home",
      "Wrangle",
      "Tidy"
    ]
  },
  {
    "objectID": "tidy.html#daten-säubern",
    "href": "tidy.html#daten-säubern",
    "title": "Tidy: tidyr",
    "section": "Daten säubern",
    "text": "Daten säubern\n\nSpalten entfernen\nUnser erstes Problem können wir bereits mit unserem Wissen aus der letzten Session lösen.\n\n\n\n\n\n\nAufgabe\n\n\n\nEntfernt die Spalte X aus dem Datensatz und speichert diesen in eurem Environment.\n\n\n\niris_messy &lt;- iris_messy %&gt;% \n                select(-X)\n\nSuper!\n\n\nSpalten umbenennen\nAuch das zweite Problem können wir mit einer Funktion aus der letzten Session lösen - denkt hier vor allem an die rename(x) Funktion.\n\n\n\n\n\n\nAufgabe\n\n\n\nÄndert alle Variablennamen in Kleinschreibung\n\n\nHier können wir uns der rename(x) Funktion aus der letzten Session bedienen, und manuell die Variablennamen ändern:\n\niris_messy %&gt;% \n  rename(variable_sepal_length = VARIABLE.SEPAL_LENGTH,\n         variable_sepal_width = VARIABLE.SEPAL_WIDTH,\n         variable_petal_length = VARIABLE.PETAL_LENGTH,\n         variable_petal_width = VARIABLE.PETAL_WIDTH)\n\n\n  \n\n\n\nDer R-Weg wäre aber eher, das Ganze programmatisch zu lösen. Dazu bieten sich die Funktionen str_to_lower(x) und rename_with(x) aus den dplyr und stringr Packages an. Deren Syntax ist wie folgt:\nMit str_to_lower(str) können wir einen String bzw. character in Kleinschreibung umwandeln:\n\nstr_to_lower(\"TEST\")\n\n[1] \"test\"\n\n\nWir sehen, die Funktion bekommt einen String, ändert diesen und gibt ihn dann Kleinschreibung zurück.\nrename_with(x) von dplyr dagegen ist quasi eine Erweiterung von rename(x), mit der wir eine Funktion zum umbenennen von Variablennamen in Dataframes verwenden können:\n\niris_messy &lt;- iris_messy %&gt;% \n                rename_with(.,str_to_lower)\n\n\n\n\n\n\n\nTipp\n\n\n\nWenn eine Funktion eigentlich ein Dataframe als Parameter erwartet (wie rename_with(dataframe, function)) und wir uns aber in einer Pipe befinden, können wir einen Punkt . anstatt des Dataframes nehmen.\n\n\nSuper! Das zweite Problem haben wir nun auch gelöst.\n\n\nSpaltennamen ändern\nLeider hat jede unserer Blattgrößen-Variablen immer noch den String variable. voranstehen. Das wollen wir ändern, und auch hierbei hilft uns stringr in Kombination mit rename_with(x). Die betreffende Funktion ist die str_replace(x) Funktion. Diese funktioniert nach folgender Syntax:\n\nstr_replace(string, pattern, replacement)\n\nWobei string der zu ändernde String ist, pattern das, wonach wir suchen und replacement unser Ersatz ist.\nHier ein konkretes Beispiel:\n\nstr &lt;- \"This is bananas\"\n\nstr_replace(str, \"is\", \"was\")\n\n[1] \"Thwas is bananas\"\n\n\n\n\n\n\n\n\nFrage\n\n\n\nWarum sieht der geänderte String nun so aus?\n\n\nDa str_replace natürlich jedes Vorkommen von “is” ersetzt, wird das “is” in “This” ersetzt. Allerdings bleibt das zweite Vorkommen von “is” nicht ersetzt. Das liegt daran, dass str_replace(x) nur das erste Vorkommen ersetzt. Wollen wir alle Vorkommen ändern, müssen wir str_replace_all(x) benutzen. Das ist in unserem Beispiel aber nicht nötig.\nKombinieren wir nun also die str_replace(x) Funktion mit der rename_with(x) Funktion, um unseren Datensatz zu ändern.\n\niris_messy &lt;- iris_messy %&gt;% \n                rename_with(., ~ str_replace(.,\"variable.\",\"\"))\n\niris_messy\n\n\n  \n\n\n\n\n\n\n\n\n\nTipp\n\n\n\nManchmal kommt es vor, dass wir eine Funktion in einer Funktion übergeben, und diese dann nochmal Parameter hat. Im Tidyverse muss man in diesen Fällen immer den Tilde-Operator ~ voranstellen.\n\n\nSuper! Nun sind die Variablennamen so, wie wir sie brauchen.\n\n\nDatentyp ändern\nNun stört uns noch, dass die Spalten mit den Blütengrößen noch als character kodiert sind. Das wollen wir ändern, dazu kennen wir ja bereits die mutate(x) Funktion aus der letzten Session und die as.numeric(x) Funktion.\n\n\n\n\n\n\nAufgabe\n\n\n\nÄndert die Datentypen der vier Blütengrößenspalten (e.g., sepal_width etc.). Speichert das Ganze noch nicht ab, sondern testet es erst (als Pipe)\n\n\n\niris_messy %&gt;% \n  mutate(sepal_length = as.numeric(sepal_length),\n         sepal_width = as.numeric(sepal_width),\n         petal_length = as.numeric(petal_length),\n         petal_width = as.numeric(petal_width))\n\nWarning: There were 4 warnings in `mutate()`.\nThe first warning was:\nℹ In argument: `sepal_length = as.numeric(sepal_length)`.\nCaused by warning:\n! NAs introduced by coercion\nℹ Run `dplyr::last_dplyr_warnings()` to see the 3 remaining warnings.\n\n\n\n  \n\n\n\nDas scheint nicht so gut geklappt zu haben. Wie wir sehen, gibt es nun viele NA Werte in den Daten.\n\n\n\n\n\n\nFrage\n\n\n\nWoran könnte das liegen?\n\n\nEin genauer Blick zeigt, dass interessanterweise genau die Werte kein NA sind, die vor der Umwandlung keine Nachkommazahl hatten. Das bedeutet, dass die as.numeric(x) Funktion offensichtlich nicht mit Nachkommazahlen klar kommt. Kann das sein?\nWohl eher nicht. Das Problem ist viel eher, dass wir das falsche Dezimalzeichen in unseren Daten haben, und zwar das , anstatt dem Punkt. Das kann manchmal passieren, wenn wir .csv Dateien mit dem deutschen Excel erstellen. Um das also zu ändern, können wir einfach wieder die str_replace(x) Funktion verwenden.\n\n\n\n\n\n\nAufgabe\n\n\n\nBenutzt die kennengelernten Funktionen, um das Komma durch einen Punkt zu ersetzen bei den Variablen für die Blütengrößen\n\n\nAuch hier hilft uns die mutate(x) Funktion:\n\niris_messy %&gt;% \n  mutate(sepal_length = str_replace(sepal_length,\",\",\".\"),\n         sepal_width = str_replace(sepal_length,\",\",\".\"),\n         petal_length = str_replace(sepal_length,\",\",\".\"),\n         petal_width = str_replace(sepal_length,\",\",\".\"))\n\n\n  \n\n\n\nSuper! Machen wir auch gleich die as.numeric(x) Funktion und speichern das Ganze ab:\n\niris_messy &lt;- iris_messy %&gt;% \n                mutate(sepal_length = str_replace(sepal_length,\",\",\".\"),\n                       sepal_width = str_replace(sepal_width,\",\",\".\"),\n                       petal_length = str_replace(petal_length,\",\",\".\"),\n                       petal_width = str_replace(petal_width,\",\",\".\")) %&gt;% \n                mutate(sepal_length = as.numeric(sepal_length),\n                       sepal_width = as.numeric(sepal_width),\n                       petal_length = as.numeric(petal_length),\n                       petal_width = as.numeric(petal_width))\n\nDas vierte Problem ist nun auch gelöst. Machen wir uns nun an das letzte Problem - die Spalte type.\n\n\n\n\n\n\nTipp\n\n\n\nAufmerksame Programmierer:innen fragen sich hier bestimmt: Geht das nicht leichter? Müssen wir wirklich jede Spalte von Hand definieren? Ja das geht leichter und natürlich nicht! Auch dafür ist in R gesorgt - das würde hier allerdings den Rahmen sprengen, weshalb wir dies zu einem späteren Zeitpunkt besprechen\n\n\n\n\nSpalten aufteilen\nWir sehen, dass die Spalte type Informationen aus den Spalten species und color enthält. Angenommen, wir wollten unseren Datensatz nach species gruppieren, so hätten wir hier ein Problem, da immer nur die Kombination aus beiden enthalten ist:\n\niris_messy %&gt;% \n  group_by(type) %&gt;% \n  summarise(sepal_length_mean = mean(sepal_length))\n\n\n  \n\n\n\nDas wollen wir nun ändern. Dazu bietet sich die separate_wider_delim(x) Funktion aus dem tidyr Package an. Diese funktioniert in einer Pipe nach folgender Syntax:\n\nseparate_wider_delim(column, separator, names)\n\nWobei column sich hier auf die betreffende Spalte bezieht, separator auf den Charakter, welcher das Trennzeichen ist (in unserem Fall - und names bezieht sich auf die Namen, die die neuen Spalten bekommen sollen.\n\n\n\n\n\n\nAufgabe\n\n\n\nWendet diese Syntax an und benennt die neuen Spalten so wie die im original Iris-Datensatz.\n\n\n\niris_messy %&gt;% \n  separate_wider_delim(type,\"-\",names = c(\"species\",\"color\"))\n\n\n  \n\n\n\nDas klappt super! Speichern wir das Ganze noch. Da wir uns sicher sind, dass der neue bereinigte iris Datensatz unserem ursprünglichen entspricht, überschreiben wir diesen auch gleich, und löschen danach den iris_messy Datensatz.\n\niris_messy &lt;- iris_messy %&gt;% \n                separate_wider_delim(type,\"-\",names = c(\"species\",\"color\"))\n\niris &lt;- iris_messy\nrm(iris_messy)\n\n\n\nNächste Session\nDas war’s mit dieser Session. Wir sind nun von unsauberen Daten nicht mehr eingeschüchtert, sondern können etwaige Probleme selbstbewusst lösen! Da wir nun also einen sauberen Datensatz haben und alle Tools zum numerischen Verständnis bereits an der Hand haben fehlt nun natürlich noch die visuelle Komponente - und darum gehts in der nächsten Session Datenvisualisierung mit ggplot.",
    "crumbs": [
      "Home",
      "Wrangle",
      "Tidy"
    ]
  },
  {
    "objectID": "packages_help.html",
    "href": "packages_help.html",
    "title": "Packages & Help",
    "section": "",
    "text": "Studentin, die realisiert, dass R durch seine Packages unschlagbar ist.",
    "crumbs": [
      "Home",
      "Program",
      "Packages & Help"
    ]
  },
  {
    "objectID": "packages_help.html#einleitung",
    "href": "packages_help.html#einleitung",
    "title": "Packages & Help",
    "section": "Einleitung",
    "text": "Einleitung\nEin großer Vorteil von R sind die zahlreichen Packages, die es für fast jeden Anwendungsfall gibt und die sehr leicht über das Repository-System CRAN (Comprehensive R Archive Network) heruntergeladen werden können. Base-R (quasi frisch nach Installation) verfügt nur über einige wenige Packages. Ein Klick auf Packages unten rechts kann euch einen Überblick über installierte & aktivierte Packages geben.\n\nBase-R ist zwar ein sehr gutes Tool mit dem wir viele Fragestellungen lösen können (Data Wrangling und Visualisierung), allerdings ist deren Anwendung manchmal etwas umständlich bzw. im Funktionsumfang dann doch eingeschränkt. Daher bietet es sich besonders an, Packages zu benutzen. Neben besonders hilfreichen und zentralen Packages wie bspw. dplyr, ggplot oder tidyr gibt es auch auf verschiedene Forschungsdomänen oder spezifische Datenanalyseaufgaben zugeschnittene Packages.\nWir behandeln in diesem Seminar vor allem solche Packages, die bei (lern)psychologischen Fragestellungen helfen. In diesem Abschnitt soll es aber nicht um spezifische Packages gehen, sondern um deren Installation und den Einsatz in R.\n\nInstallation\nWir können Packages über die Funktion install.packages(x) installieren. Hier ist wichtig zu verstehen, dass wir dies nur einmal machen müssen - danach sind die Packages ja installiert. Daher sollte man den Befehl install.packages(x) auch nicht in einem Code Chunk ausführen, sondern direkt in der R-Konsole, und zwar nach der Syntax install.packages(\"package_name\"). Hier ist wichtig, dass der Name des Packages in Anführungszeichen \" ist, da wir ihn der Funktion als character übergeben.\n\n\n\n\n\n\n\nAufgabe\n\n\n\nInstalliert nun nach diesem Prinzip das Package tidyverse.\n\n\nMit tidyverse habt ihr bereits eine Vielzahl der wichtigsten Packages installiert (es ist quasi eine Sammlung an Packages). Hier findet ihr eine Übersicht über die installierten Packages. Wir werden am meisten mit dplyr, ggplot2, tidyr und lubridate arbeiten, aber auch andere Packages verwenden.\n\n\nAktivierung\nSuper! Nun ist tidyverse installiert. Allerdings müssen wir das Package noch aktivieren. Dazu brauchen wir den library(x) Befehl, und führen ihn entsprechend aus:\n\nlibrary(tidyverse)\n\n\n\n\n\n\n\nFrage\n\n\n\nWas ist beim Aktivieren der Packages anders als beim Installieren?\n\n\n\n\nStruktur\nNachdem wir Packages aktiviert haben, können wir ihre Funktionen während der gesamten R-Session verwenden. Es bietet sich an, die Packages gleich zu Beginn zu aktivieren, d.h., am besten gleich zu Anfang eures Codes, damit alle nachfolgenden Code-Blöcke auf das Package zugreifen können.\nDa wir im Rahmen dieses Seminars mit Quarto arbeiten, bietet es sich an, als ersten Chunk einen sogenannten Preliminaries Chunk anzulegen, indem ihr alle benötigten Packages aktiviert, und nebenbei auch alle anderen wichtigen DInge definiert, wie bspw. benötigte Farbpaletten, benutzerdefinierte Funktionen (später mehr zu beidem). Folgende Grafik veranschaulicht das etwas:",
    "crumbs": [
      "Home",
      "Program",
      "Packages & Help"
    ]
  },
  {
    "objectID": "packages_help.html#help",
    "href": "packages_help.html#help",
    "title": "Packages & Help",
    "section": "Help",
    "text": "Help\nEs kann immer mal vorkommen, dass wir bei manchen Funktionen HIlfe brauchen. Dazu gibt es in R die help(x) Funktion. Angenommen, ich brauche Hilfe bei der round(x) Funktion, da ich nicht mehr genau weiß, wie deren Parameter sind. Die Syntax ist:\nhelp(function)\nAlso schreiben wir:\n\nhelp(round)\n\nUnd es öffnet sich unten rechts in R-Studio der Hilfereiter:\n\nAlternativ können wir auch schreiben:\n\n?help()\n\nIm Hilfereiter befindet sich dann die Dokumentation. Alternativ wäre diese auch im Web zu finden. Nach gleichem Prinzip können wir auch die Dokumentation ganzer Packages beziehen:\n\nhelp(tidyverse)\n\n\nHilfe außerhalb von R-Studio\nAuch außerhalb von R-Studio könnt ihr euch Hilfe holen. Moderne Programmiersprachen und vor allem R sind heutzutage so gut dokumentiert, dass ihr nach wenigen Suchbegriffen schon eine Lösung für euer Problem gefunden habt. Falls nicht gibt es bestimmt jemanden, der das gleiche Problem wie ihr hat, und das auf Stackoverflow nachgfragt hat. Oder ihr macht es wie jede:r in 2024 und benutzt LMMs wie ChatGPT - seid hier aber gewarnt und denkt an unsere Hinweise aus der Präsentation! Die Antworten von LLMs sind nicht immer akkurat und vor allem nehmen sie uns die Fähigkeit, unser Problem zu abstrahieren. Das ist eine wichtige Eigenschaft, die ihr lernen solltet.\n\n\n\nRessourcen wenn es um Hilfe bei R geht: Google, Stackoverflow oder auch ChatGPT\n\n\n\n\nNächste Session\nDas war’s mit dieser Session. In der nächsten Session geht es um den Import von Daten.",
    "crumbs": [
      "Home",
      "Program",
      "Packages & Help"
    ]
  },
  {
    "objectID": "base_r.html",
    "href": "base_r.html",
    "title": "Wrangle: Base R",
    "section": "",
    "text": "Student, der gerade Data Wrangling macht",
    "crumbs": [
      "Home",
      "Wrangle",
      "Base R"
    ]
  },
  {
    "objectID": "base_r.html#einleitung",
    "href": "base_r.html#einleitung",
    "title": "Wrangle: Base R",
    "section": "Einleitung",
    "text": "Einleitung\nR bietet bereits ohne installierte Packages eine Vielzahl an Möglichkeiten, mit Daten zu hantieren. Im Rahmen dieses Seminars werden einen Fokus auf das Data Wrangling mit Packages wie tidyR & dplyr legen - dennoch ist es wichtig, auch die grundlegenden Base-R Funktionen & Möglichkeiten zu verstehen. Dazu soll dieses Kapitel dienen.",
    "crumbs": [
      "Home",
      "Wrangle",
      "Base R"
    ]
  },
  {
    "objectID": "base_r.html#daten-verstehen",
    "href": "base_r.html#daten-verstehen",
    "title": "Wrangle: Base R",
    "section": "Daten verstehen",
    "text": "Daten verstehen\n\nDatensatz erneut einlesen\nFalls der Datensatz aus der letzten Session nicht mehr im Environment ist, lesen wir ihn kurzerhand neu ein:\n\niris_csv &lt;- read_csv(\"assets/datasets/iris/iris.csv\")\n\nDer Einfachheit halber wollen wir den Datensatz allerdings in iris umbennen, da uns die anderen Datensätze (iris_sav etc.) nicht interessieren.\n\n\n\n\n\n\nAufgabe\n\n\n\nBenennt den iris_csv Datensatz in iris um\n\n\n\niris &lt;- iris_csv\nrm(iris_csv)\n\n\n\n\n\n\n\nTipp\n\n\n\nMit der rm(x) Funktion kann ich Objekte und Werte aus unserem Environment löschen, um für etwas Ordnung zu sorgen\n\n\n\n\nDatenlage verstehen\nUm einen schnellen Überblick über die Daten zu bekommen, bietet sich die head(x) Funktion an, mit der wir uns die ersten Zeilen eines Datensatzes anschauen können.\n\nhead(iris)\n\n\n  \n\n\n\nHier sehen wir also die verschiedenen Variablen, sowie deren Ausprägungen in den ersten 6 Fällen. Wollen wir wissen wieviele Zeilen im Datensatz vorhanden sind, benutzen wir nrow(x):\n\nnrow(iris)\n\n[1] 150\n\n\nNun interessieren uns noch generelle Infos über die Variablen. Dazu haben wir in der letzten Session haben\nbereits die summary(x) Funktion kennengelernt:\n\nsummary(iris)\n\n  sepal_length    sepal_width     petal_length    petal_width   \n Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100  \n 1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300  \n Median :5.800   Median :3.000   Median :4.350   Median :1.300  \n Mean   :5.843   Mean   :3.054   Mean   :3.759   Mean   :1.199  \n 3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800  \n Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500  \n   species         \n Length:150        \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n\n\nHier sehen wir erste deskriptive Werte für die numerischen Variablen, sowie nicht-numerischen Variablen, wie in dem Fall species. Hier sehen wir allersdings nicht die Ausprägungen. Über die Funktion levels(x) können wir uns hier aber die verschiedenen Stufen des Faktors anschauen.\n\n\n\n\n\n\nAufgabe\n\n\n\nBenutzt die levels(x) Funktion für die Spalte species in unserem Datensatz.\n\n\n\nlevels(iris$species)\n\nNULL\n\n\n\n\n\n\n\n\nFrage\n\n\n\nWarum bekommen wir hier NULL zurückgegeben?\n\n\nWir bekommen NULL zurückgegeben, da die Variable nur den Datentyp numeric hat, nicht jedoch als Faktor gespeichert ist. Das hätte uns bei einer genaueren Betrachtung bereits beim Output der summary(x) Funktion auffallen können. Um das nun aber zu ändern, haben wir in einer der letzten Sessions bereits eine Funktion kennengelernt.\n\n\n\n\n\n\nAufgabe\n\n\n\nÄndert den Datentyp der species Spalte mit einer Funktion aus der letzten Session\n\n\n\niris$species &lt;- as.factor(iris$species)\n\nUnd siehe da - nun bekommen wir auch die Faktorstufen angezeigt:\n\nlevels(iris$species)\n\n[1] \"setosa\"     \"versicolor\" \"virginica\" \n\n\n\n\n\n\n\n\nAchtung\n\n\n\nCheckt nach dem Import immer nochmal, ob alle Spalten auch den richtigen Datentyp haben.\n\n\nManchmal wird der Output der summary(x) Funktion etwas unübersichtlich. Wir können die numerischen Spalten aber auch ganz schnell mit einer Reihe eingebauter R-Funktionen deskriptiv betrachten:\nEtwa für den Mittelwert:\n\nmean(iris$sepal_length)\n\n[1] 5.843333\n\n\nDie Standardabweichung:\n\nsd(iris$sepal_length)\n\n[1] 0.8280661\n\n\nDie Min/Max-Werte:\n\nmin(iris$sepal_length)\n\n[1] 4.3\n\nmax(iris$sepal_length)\n\n[1] 7.9\n\n\nQuantile:\n\nquantile(iris$sepal_length)\n\n  0%  25%  50%  75% 100% \n 4.3  5.1  5.8  6.4  7.9 \n\n\nUnd vieles mehr. Über help(base) finden wir dazu mehr Informationen.\n\n\nDaten transformieren\nHäufig wollen wir einen Datensatz transformieren, etwa indem wir Spalten hinzufügen oder entfernen. Das können wir mithilfe des $-Operators in Kombination mit dem Zuweisungsoperator.\n\n\n\n\n\n\nAufgabe\n\n\n\nErstellt ein neues Objekt namens dataset_species, welches nur die Spalte species aus unserem Datensatz enthält.\n\n\n\ndataset_species &lt;- iris$species\n\n\n\n\n\n\n\nFrage\n\n\n\nWelchen Typ hat das resultierende Objekt/Wert?\n\n\nRichtig, wir haben nun wieder einen Vektor. Das unterstützt unsere usprüngliche Aussage, dass Dataframes intern eigentlich aus Vektoren bestehen. Über length(x) können wir überprüfen, ob der Vektor gleich lang ist, wie unser Dataframe auch Beobachtungen hat:\n\nlength(dataset_species)\n\n[1] 150\n\n\nVoilá! Das hat also geklappt. Angenommen, wir wollen nun nicht nut spezifische Spalten aus unserem Datensatz haben, sondern auf Basis bestimmter Kriterien filtern. Dazu gibt es die subset(x) Funktion.\n\n\nDaten auswählen\nWenn wir bspw. nur Datenpunkte mit einer sepal_width &gt; 3 betrachten wollen, können wir dies über subset(x) lösen:\n\niris_filtered &lt;- subset(iris, sepal_width &gt; 3)\n\nUnser neues Dataframe iris_filtered enthält nun nur solche Zeilen, welche eine sepal_width &gt; 3 haben. Wir können auch Bedingungen kombinieren, bspw. um nur Datenpunkte der Spezies setosa zu betrachten:\n\niris_filtered2 &lt;- subset(iris, sepal_width &gt; 3 & species == \"setosa\")\n\n\n\n\n\n\n\nAufgabe\n\n\n\nErstellt ein Objekt iris_filtered3, welches alle Datenpunkte, außer die der Spezies virginica enthält.\n\n\n\niris_filtered3 &lt;- subset(iris, species != \"virginica\")\n\nWir haben nun also gelernt, wie wir sogenannte Subsets von Dataframes auf Basis von Variablenausprägungen und Spalten erzeugen können, und diese enstprechend in unserem Environment speichern können.\n\n\n\n\n\n\nFrage\n\n\n\nWelche Frage stellt sich, wenn wir nun unser Environment betrachten?\n\n\n\nRichtig! Hier herrscht ein ganz schönes Durcheinander. Nach einer längeren R-Session sieht das wahrscheinlich noch schlimmer aus. Wie wir das ganze übersichtlicher, effizienter und einfacher lösen können, lernen wir in der nächsten Session.",
    "crumbs": [
      "Home",
      "Wrangle",
      "Base R"
    ]
  },
  {
    "objectID": "assignments.html",
    "href": "assignments.html",
    "title": "Übungsblätter & Lösungen",
    "section": "",
    "text": "Nr.\nÜbungsblatt\nLösungen\n\n\n\n\n1\n\n\n\n\n2"
  },
  {
    "objectID": "model.html",
    "href": "model.html",
    "title": "Model",
    "section": "",
    "text": "Student, der gerade mit R Daten inferenzstatistisch untersucht",
    "crumbs": [
      "Home",
      "Model",
      "Model"
    ]
  },
  {
    "objectID": "model.html#die-qual-der-wahl",
    "href": "model.html#die-qual-der-wahl",
    "title": "Model",
    "section": "Die Qual der Wahl",
    "text": "Die Qual der Wahl\nÄhnlich wie in den vorigen Sessions erläutert, gibt es ähnlich zu den Unterschieden zwischen Base-R und dplyr auch bei den inferenzstatistischen Verfahren verschiedene Packages und Funktionen.\nGrundsätzlich gibt es hier keine richtigen oder falschen Packages, stattdessen hängt es vom Kontext ab. Manche Packages ermöglichen ziemlich komplexe Analysen, und um sich die als Möglichkeit offen zu lassen, kann es sich lohnen, auch direkt die “einfachen” Analysen eines Projekts damit zu rechnen, damit die verschiedenen Analysen kompatibler miteinander sind. Manche Packages sind von der Syntax möglichst eingängig gestaltet, sodass man als Anfänger besser abgeholt wird. Wiederum andere bieten die gleichen Funktionen, verwenden aber leicht unterschiedliche Berechnungsmethoden, da die Methoden für verschiedene Kontexte verschieden robust sind.\nMeistens macht es Sinn mit dem, was man kennt (bzw. was ihr hier kennenlernt), anzufangen. Wenn sich dann eine Datensituation ergibt, die komplexer ist, lässt sich immer noch im Internet recherchieren, welches Package/welche Funktion vielleicht besser geeignet sind. Die Hürde wird dann weniger euer R-Wissen sein, sondern eher euer allgemeines Statistik-Wissen. Unserer Erfahrung nach verbringt man eher mehr Zeit mit der Recherche statistischer Methoden anstatt mit der tatsächlichen Anwendung von R-Funktionen.\nIm Rahmen dieser Session zeigen wir euch die Funktionen aus dem rstatix Package, welches standard Base-R Funktionenn wir bspw. t.test(x) Pipe-freundlich macht, und somit direkt im Anschluss an dplyr Pipes verwendet werden kann. In Fällen wo wir doch Base-R Funktionen verwenden, machen wir das kenntlich.",
    "crumbs": [
      "Home",
      "Model",
      "Model"
    ]
  },
  {
    "objectID": "model.html#installationaktivierung",
    "href": "model.html#installationaktivierung",
    "title": "Model",
    "section": "Installation/Aktivierung",
    "text": "Installation/Aktivierung\nZunächst müssen wir rstatix aktivieren, da es nicht im tidyverse enthalten ist:\n\nlibrary(rstatix)\n\n\n\n\n\n\n\nHinweis\n\n\n\nFalls rstatix noch nicht installiert ist, tut dies über install.packages(\"rstatix\") in der R-Konsole",
    "crumbs": [
      "Home",
      "Model",
      "Model"
    ]
  },
  {
    "objectID": "model.html#datensätze",
    "href": "model.html#datensätze",
    "title": "Model",
    "section": "Datensätze",
    "text": "Datensätze\nHier noch ein paar Worte zu den Datensätzen.\n\nDer flights Datensatz beinhaltet Daten zu Verspätungen aller Flüge die in New York City (i.e., JFK, LGA or EWR) im Jahr 2013 gestartet sind.\nDer cars Datensatz beinhaltet Geschwindigkeiten und Bremswege von Autos\nDer ChickWeight Datensatz beinhaltet Informationen zum Gewicht vs. Alter von Hühnern auf verschiedenen Diäten\nDer ToothGrowth Datensatz enthält Informationen Zahnwachstum von 60 Meerschweinchen\nDer swiss Datensatz enthält Daten zu Fruchtbarkeit in 47 schweizer Provinzen",
    "crumbs": [
      "Home",
      "Model",
      "Model"
    ]
  },
  {
    "objectID": "model.html#voraussetzungen",
    "href": "model.html#voraussetzungen",
    "title": "Model",
    "section": "Voraussetzungen",
    "text": "Voraussetzungen\nWie ihr wisst, gibt es für die sogennannten parametrischen Testverfahren bestimmte Voraussetzungen, die die Daten erfüllen sollten, damit die Tests anwendbar sind und die Auswertung angemessen. Vorbildlich wie wir sind, schauen wir uns diese Tests für die Voraussetzungen zuerst an.\n\nNormalverteilung\nUm Rückschlüsse auf die Grundgesamtheit zu ziehen und unsere Daten zu generalisieren, sind normalverteilte Daten wichtig. Zur Erinnerung: Wir wollen, dass unsere Daten in etwa dieser Verteilung gleichen:\n\n\nCode\nnormal_data &lt;- data.frame(x = c(-3, 3)) \n\nnormal_data %&gt;% \n  ggplot(aes(x)) +\n  stat_function(fun = dnorm,\n                geom = \"line\")+\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nWenn wir uns an unseren iris Datensatz erinnern, so sah dessen Verteilung ziemlich ähnlich aus:\n\nggplot(data=iris)+\n  geom_bar(aes(x=sepal_width, fill=species), color=\"#2b2b2b\")+\n  ylab(\"Prevalence\")+\n  xlab(\"Width of Sepal\")+\n  ggtitle(\"Sepal width of Iris Flower for each species\")+\n  scale_fill_manual(values = metro_colors)+\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHinweis\n\n\n\niris Datensatz einlesen, falls nicht mehr im Environment: iris &lt;- read_csv(\"assets/datasets/iris/iris.csv\")\n\n\nNatürlich müssen wir das noch statistisch überprüfen. Dazu bietet sich der Shapiro-Test an. Dieser testet, ob die Daten signifikant von einer Normalverteilung abweichen. Wenn er signifikant wird, sind die Daten also nicht normalverteilt. Die Syntax ist simpel:\n\ndataset %&gt;% \n  shapiro_test(variable)\n\n\n\n\n\n\n\nAufgabe\n\n\n\nFührt den Shapiro-Test für die sepal_width Variable durch.\n\n\nTesten wir das also für die sepal_width Variable:\n\niris %&gt;% \n  shapiro_test(sepal_width)\n\n\n  \n\n\n\n\n\n\n\n\n\nTipp\n\n\n\nrstatix ist quasi ein Wrapper für die Base-R Funktionen, der sie Pipe-freundlich macht (Input & Output). Mit Base-R würde der obige Code & Output so aussehen:\n\nshapiro.test(iris$sepal_width)\n\n\n    Shapiro-Wilk normality test\n\ndata:  iris$sepal_width\nW = 0.98379, p-value = 0.07518\n\n\n\n\nVoilá! Die Daten scheinen normalverteilt zu sein. Trotzdem ist der p Wert nur knapp nicht signifikant. Das hat einen Grund:\nJe größer jedoch die Stichprobe ist, desto schlechter funktioniert der Shapiro-Wilk Test, da er dann tendenziell zu schnell signifikant wird, obwohl die Daten einigermaßen normalverteilt sind. Daher bietet es sich an, ab ca. N &gt; 50 stattdessen einen QQ-Plot anzuschauen. Ab n &gt; 5000 würde der Test nicht mehr ausgeführt werden.\nHier eine Demonstration, wie ein QQ-Plot mit normalverteilten Daten aussehen sollte. Dazu erstellen wir über rnorm(x) zunächst normalverteilte Daten:\n\n# normalverteilte Werte erzeugen\nvec.norm &lt;- rnorm(100, mean = 0, sd=1)\n\nUnd visualisieren diese dann mit geom_qq(x):\n\n# QQ-Plot\nggplot()+\n  geom_qq(aes(sample=vec.norm))+\n  theme_minimal()\n\n\n\n\n\n\n\n\nBei einem QQ-Plot werden die Werte, die wir testen, standardisiert und dann gegen die “echte” Standardnormalverteilung geplottet. Wenn unsere Daten perfekt (standard)normalverteilt wären, würde sich eine perfekte 45°-Gerade ergeben. In unserem Fall sieht das größtenteils normalverteilt aus. Auch der Shapiro Test auf den normalverteilten Daten ist viel weniger nah an der Signifikanz:\n\nvec.norm %&gt;% \n  shapiro_test()\n\n\n  \n\n\n\nUnsere künstlichen Daten oben sind auf jeden Fall nah genug an der “perfekten Gerade” dran.\n\n\n\n\n\n\nFrage\n\n\n\nWie sieht es mit den iris Daten aus?\n\n\n\niris %&gt;% \n  ggplot()+\n    geom_qq(aes(sample=sepal_width))+\n    theme_minimal()\n\n\n\n\n\n\n\n\nAuch hier sieht es nach einer Normalverteilung aus. Falls die Daten nicht normalverteilt wären, würde dieses Plot anders aussehen.\n\n\n\n\n\n\nFrage\n\n\n\nWie würde die Verteilung aussehen, wenn die Daten nicht normalverteilt wären? Nutzt einen der anderen Datensätze\n\n\n\nChickWeight %&gt;% \n   ggplot()+\n    geom_qq(aes(sample=weight))+\n    theme_minimal()\n\n\n\n\n\n\n\n\nHier sehen wir keine Normalverteilung, da die Punkte keiner 45° Kurve entsprechen.\n\n\nVarianzhomogenität\nEine weitere Voraussetzung, die wir überprüfen müssen, ist die Varianzhomogenität. Dabei geht es darum, dass unterschiedliche Gruppen bezüglich einer Variable die in etwa gleiche Varianz haben. Dazu gibt es in R die levene_test(x) Funktion. Testen wir das gleich mit dem flights Datensatz für die dep_delay Variable (Verspätung bei Abflug):\n\nflights %&gt;% \n  levene_test(dep_delay ~ origin)\n\n\n  \n\n\n\nWie wir sehen, ist unser p Wert sehr klein, d.h., wir müssen von inhomogenen Varianzen ausgehen.",
    "crumbs": [
      "Home",
      "Model",
      "Model"
    ]
  },
  {
    "objectID": "model.html#gruppenvergleiche",
    "href": "model.html#gruppenvergleiche",
    "title": "Model",
    "section": "Gruppenvergleiche",
    "text": "Gruppenvergleiche\nEines der häufigsten Anwendungsfelder im inferenzstatistischen Bereich sind Gruppenvergleiche. Wie das in R geht, schauen wir uns nun an:\n\nt-Test\nWir wollen zwei Gruppen vergleichen und unsere Datenstruktur ist eine kontinuierliche abhängige Variable & eine dichotome Gruppenvariable.\nZur Veranschaulichung verwenden wir hier den Datensatz ToothGrowth: Hier wurde der Zahnwachstum von Meerschweinchen in Abhängigkeit von verabreichtem Vitamin C untersucht. Die Stichprobe besteht aus 60 Meerschweinchen und sowohl die Verabreichungsmethode (Orangensaft vs. Ascorbinsäure) als auch die Vitamin C-Dosis (0.5, 1 oder 2mg pro Tag) wurden variiert. Da wir beim t-Test nur mit zwei Gruppen arbeiten können, schauen wir uns jetzt nur die Unterschiede durch die Verabreichungsmethode supp an.\nZunächst müssen wir die Faktor-variablen auch als Faktor umwandeln. Wir nennen den neuen Datensatz toothgrowth (kleingeschrieben):\n\ntoothgrowth &lt;- ToothGrowth %&gt;% mutate(supp = as.factor(supp),\n                                      dose = as.factor(dose))\n\n\n\n\n\n\n\nHinweis\n\n\n\ndose ist ohne diese Umwandlung numeric, was später zu Problemen führen kann.\n\n\nHier ein Überblick über die Daten:\n\ntoothgrowth %&gt;% \n  get_summary_stats()\n\n\n  \n\n\n\n\n\n\n\n\n\nTipp\n\n\n\nget_summary_stats(x) ist quasi das Pipe-Äquivalent zu summary(x)\n\n\nÜber dplyr können wir schnell auf Unterschiede in der Länge schauen in Abhängigkeit von supp.\n\n\n\n\n\n\nAufgabe\n\n\n\nBerechnet den Mittelwertsunterschied für len in Abhängigkeit von supp mit Hilfe von group_by(x)\n\n\n\ntoothgrowth %&gt;% \n  group_by(supp) %&gt;% \n  summarise(mean=mean(len))\n\n\n  \n\n\n\nDeskriptiv sehen wir einen Unterschied. Schauen wir uns den Unterschied hinsichtlich des Zahnwachstums auch als Boxplot an:\n\nggplot(toothgrowth)+\n  geom_boxplot(aes(x=supp, y=len,fill=supp))+\n  scale_fill_manual(values=metro_colors)+\n  theme_minimal()\n\n\n\n\n\n\n\n\nAuch hier sehen wir den Unterschied schön. Das wollen wir nun inferenzstatistisch testen.\n\nSyntax\nÄhnlich wie beim levene_test(x) oben ist der Aufbau beim t-Test hier wie folgt:\n\ndata %&gt;% \n  t_test(dependent_variable ~ group)\n\nWir sehen, dass wir über den Tilde-Operator ~ definieren, was die AV (dependent_variable) und was die UV (group) ist. In R wird er häufig verwendet, um einen Zusammenhang, eine Abhängigkeit oder ein Modell zu symbolisieren.\nNatürlich gibt es innerhalb von t_test() mit Argumenten Möglichkeiten um\n\neinen t-Test für eine Stichprobe durchzuführen: z.B. mu = 100\neinen t-Test für abhängige Stichproben durchzuführen: paired = TRUE\neinen gerichteten t-Test durchzuführen: z.B. alternative = \"less\"\nbei nicht vorliegender Varianzhomogenität stattdessen den Welch-Test anzuwenden: var.equal = FALSE\n\n\n\nAnwendung\nWenden wir unser Wissen nun auf den ToothGrowth Datensatz an:\n\ntoothgrowth %&gt;% \n  t_test(len ~ supp,var.equal = TRUE)\n\n\n  \n\n\n\n\n\n\n\n\n\nHinweis\n\n\n\nDie rstatix Funktionen heißen netterweise fast immer exakt so wie die äquivalenten Base-R Funktionen, nur dass statt . ein _ verwendet wird: t.test() vs. t_test().\n\n\n\n\nNonparametrisch\nFalls die Voraussetzungen nicht erfüllt sein sollten, so gibt es natürlich auch nonparametrische Alternativen wie den Wilcoxon Rank Sum Test:\n\ntoothgrowth %&gt;% \n  wilcox_test(len ~ supp)\n\n\n  \n\n\n\n\n\n\nANOVA\nWenn wir zwei oder mehr Gruppen miteinander vergleichen wollen, rechnen wir eine ANOVA. Zur Veranschaulichung können wir hier bei den Meerschweinchendaten bleiben, da die Dosis-Variable drei Faktorstufen hat:\n\nlevels(toothgrowth$dose)\n\n[1] \"0.5\" \"1\"   \"2\"  \n\n\n\n\n\n\n\n\nTipp\n\n\n\nWir sehen, es lohnt sich manchmal, auch Base-R Syntax zu verwenden, wenn wir schnell an Infos kommen wollen\n\n\nRechnen wir die ANOVA:\n\ntoothgrowth %&gt;%\n  anova_test(len ~dose)\n\n\n  \n\n\n\nDie Funktion anova_test() gibt uns direkt das erwartete Ergebnis aus und zeigt an, dass es signifikante Mittelwertsunterschiede gibt.\n\n\n\n\n\n\nTipp\n\n\n\nBase-R würde hier wie folgt funktionieren:\n\ntooth.aov &lt;- aov(len ~ dose, toothgrowth)\nsummary(tooth.aov)\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \ndose         2   2426    1213   67.42 9.53e-16 ***\nResiduals   57   1026      18                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nDie Funktion gibt uns nicht direkt das Ergebnis, was wir von einer ANOVA erwarten, sondern fittet (=“baut”) erst mal nur das Modell. Wenn wir das ANOVA-Modell auswerten wollen, müssen wir uns das aov-Ergebnis über summary() zusammenfassen lassen\n\n\n\nKomplexere Modelle\nKomplexere Modelle mit mehreren Faktoren lassen sich natürlich auch realisieren. Dafür ein neues Beispiel: Im Datensatz ChickWeight wurde das Gewicht von Küken seit Geburt getrackt. Es sind verschiedene Messzeitpunkte und die Art der Ernährung enthalten.\n\nChickWeight %&gt;% tibble()\n\n\n  \n\n\nChickWeight %&gt;% \n  get_summary_stats()\n\n\n  \n\n\n\nHier schauen wir uns direkt nur rstatix an, da das die eindeutig angenehmere Umsetzung ist.\n\nChickWeight %&gt;% \n  anova_test(weight ~ Diet + Time + Error(Chick/Time))\n\nANOVA Table (type III tests)\n\n$ANOVA\n     Effect DFn DFd       F         p p&lt;.05   ges\n1      Diet   3  41   5.075  4.00e-03     * 0.161\n2      Time  11 451 280.945 6.41e-194     * 0.769\n3 Diet:Time  33 451   3.766  9.34e-11     * 0.118\n\n$`Mauchly's Test for Sphericity`\n     Effect        W         p p&lt;.05\n1      Time 2.68e-17 1.03e-251     *\n2 Diet:Time 2.68e-17 1.03e-251     *\n\n$`Sphericity Corrections`\n     Effect   GGe      DF[GG]    p[GG] p[GG]&lt;.05   HFe      DF[HF]    p[HF]\n1      Time 0.114 1.26, 51.48 2.01e-24         * 0.116 1.28, 52.34 8.63e-25\n2 Diet:Time 0.114 3.77, 51.48 1.00e-02         * 0.116 3.83, 52.34 1.00e-02\n  p[HF]&lt;.05\n1         *\n2         *\n\n\n\n\n\n\n\n\nTipp\n\n\n\nEs gibt in rstatix auch eine alternative Schreibweise, die manche vllt. besser finden:\n\nChickWeight %&gt;% \n  anova_test(dv = weight,\n             between = Diet,\n             within = Time,\n             wid = Chick)\n\n\n\nBei Daten mit Messwiederholung müssen wir spezifizieren, welcher Faktor mehrmals gemessen wurde und an welcher Variable erkannt wird, welche Messungen zu welchem “Probanden” gehören. Das lässt sich durch den Term Error(ProbandenID/wiederholterFaktor) ausdrücken.\nanova_test() testet netterweise die Sphärizität direkt mit. Wenn der Mauchly-Test signifikant wird, müssen wir die messwiederholten Faktoren (hier: Time und die Interaktion Diet:Time) im unteren Abschnitt des Outputs interpretieren. Die Spalten mit “GG” sind dabei korrigiert nach Greenhouse-Geisser, die Spalten mit “HF” nach Huynh-Feldt.\nKovariaten können wir in anova_test() über das Argument covariate = einfügen, so wie bei dv =, between = usw.\nNoch mehr Faktoren/Variablen lassen sich über c(x) verknüpfen, z.B. between = c(Diet, Species)\n\n\nPost Hoc Test\nWenn eine ANOVA signifikant wird, interessiert uns meistens noch, welche/r der Mittelwertsunterschiede dafür verantwortlich ist/sind. In Bezug auf unser Meerschweinchen-Beispiel hängt das Zahnwachstum len offensichtlich von der Dosis dose ab, aber bisher wissen wir nicht, ob die höchste Dosis zu mehr Wachstum als die anderen beiden führt, oder ob sich alle signifikant voneinander unterscheiden, oder ob es nur einen Unterschied im Bezug zur niedrigsten Dosis gibt usw.\n\ntoothgrowth %&gt;%\n  tukey_hsd(len ~ dose)\n\n\n  \n\n\n\nDie Funktion tukey_hsd erlaubt als Input entweder eine Formel (wie hier) oder das Ergebnis von aov() oder lm(). Der Output von anova_test() funktioniert hier nicht als Input!\n\n\nNonparametrisch: Kruskal-Wallis Rank Sum Test\nSollten die Voraussetzungen nicht erfüllt sein, können wir den Kruskal-Wallis Test nach gleicher Syntax rechnen:\n\n# rstatix\ntoothgrowth %&gt;% \n  kruskal_test(len ~ dose)",
    "crumbs": [
      "Home",
      "Model",
      "Model"
    ]
  },
  {
    "objectID": "model.html#zusammenhänge",
    "href": "model.html#zusammenhänge",
    "title": "Model",
    "section": "Zusammenhänge",
    "text": "Zusammenhänge\nNeben Mittelwertsvergleichen bzw. Gruppenvergleichen sind ein weiterer wichtiger Punkt, der uns interessiert Zusammenhänge.\n\nKorrelation\nWenn wir den Zusammenhang zwischen zwei Variablen feststellen wollen und zwei kontinuierliche Variablen haben, können wir zunächst die Korrelation berechnen.\nFür dieses Beispiel haben wir den cars Datensatz, der Geschwindigkeit und Bremsweg von Autos enthält.\n\ncars\n\n\n  \n\n\nget_summary_stats(cars)\n\n\n  \n\n\nggplot(cars)+\n  geom_point(aes(speed, dist))+\n  theme_minimal()\n\n\n\n\n\n\n\n\n\ncars %&gt;% \n  cor_test(speed, dist)\n\n\n  \n\n\n\nBase R kann Korrelationen berechnen, hat aber standardmäßig keinen Test auf Signifikanz enthalten. Hier wird der Nutzen von rstatix besonders deutlich, und wir sehen dass die cor_test(x) Funktion uns hier eine signifikante Korrelation von .81 zeigt.\n\nNonparametrisch: Spearman & Kendall\nNonparametrische Korrelationsberechnungen lassen sich über das Argument method spezifizieren.\n\ncars %&gt;% \n  cor_test(speed, dist, method = \"spearman\")\n\n\n  \n\n\n\n\n\n\nRegression\nWenn wir einen Schritt weitergehen wollen, und Werte vorhersagen, relevante Prädiktoren für eine bestimmte abhängige Variable identifizieren und das Modell mit bestem Fit finden wollen, bietet sich eine Regression an. Auch das ist in R kinderleicht.\nrstatix hilft uns hier leider nicht mehr weiter, aber Base-R funktioniert hier auch sehr gut.\nDie Datenstruktur sollte hier eine kontinuierliche abhängige Variable & kategoriale/kontinuierliche Prädiktoren beinhalten.\nHier haben wir als Datenbeispiel den Datensatz swiss, in dem die Fertilität der Population der 47 französisch-sprachigen Provinzen in der Schweiz erfasst wurde. Die weiteren Variablen:\n\nAgriculture: % of males involved in agriculture as occupation\nExamination: % draftees receiving highest mark on army examination\nEducation: % education beyond primary school for draftees\nCatholic: % ‘catholic’ (as opposed to ‘protestant’)\nInfant.Mortality: live births who live less than 1 year\n\nSchauen wir uns das an:\n\nswiss\n\n\n  \n\n\n\n\n\n\n\n\n\nAufgabe\n\n\n\nBerechnet den Zusammenhang zwischen Fertility und weiteren Variablen. Was fällt auf?\n\n\n\nswiss %&gt;% \n  cor_test(Fertility,Agriculture)\n\n\n  \n\n\n\nWir sehen Zusammenhänge und wollen nun ein multiples Regressionsmodell rechnen. Die Syntax dafür ist wie folgt:\n\nmodel &lt;- lm(criterion ~ predictor1+predictor2+predictor3,dataset)\n\n\n\n\n\n\n\nHinweis\n\n\n\nUm den Output sinnvoll interpretieren zu können, müssen wir den Output unserer lm(x) Funktion in einem Objekt speichern. Dort sind alle relevanten Daten enthalten, und wir können es dann später über die summary(x) Funktion verwenden.\n\n\nLegen wir nun also mit dem swiss Datensatz los:\n\nswiss.lm &lt;- lm(Fertility ~ Agriculture + Examination + Education + Catholic, swiss)\nsummary(swiss.lm)\n\n\nCall:\nlm(formula = Fertility ~ Agriculture + Examination + Education + \n    Catholic, data = swiss)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.7813  -6.3308   0.8113   5.7205  15.5569 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 91.05542    6.94881  13.104  &lt; 2e-16 ***\nAgriculture -0.22065    0.07360  -2.998  0.00455 ** \nExamination -0.26058    0.27411  -0.951  0.34722    \nEducation   -0.96161    0.19455  -4.943 1.28e-05 ***\nCatholic     0.12442    0.03727   3.339  0.00177 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.736 on 42 degrees of freedom\nMultiple R-squared:  0.6498,    Adjusted R-squared:  0.6164 \nF-statistic: 19.48 on 4 and 42 DF,  p-value: 3.95e-09\n\n\nMit diesem Output können wir feststellen, welche Prädiktoren signifikant mit der Fertilität zusammenhängen, kontrolliert für die anderen Prädiktoren. Auch hier sehen wir wieder: lm(x) baut nur das Modell, erst summary(x) wertet es aus. In dem Fall gibt es kein Äquivalent von rstatix.\nWie oben erwähnt, kann ein anderes mögliches Ziel einer Regression sein, das Regressionsmodell mit dem besten Fit zu den Daten herauszufinden, also nur Prädiktoren nach dem Prinzip “so viel wie nötig, so wenig wie möglich” im finalen Modell zu behalten. Dafür werden mehrere Regressionsgleichungen bezüglich ihrer Varianzaufklärung miteinander verglichen.\n\n# Modelle bauen\nswiss.lm1 &lt;- lm(Fertility ~ 1, swiss)\nswiss.lm2 &lt;- update(swiss.lm1, ~. + Education)\nswiss.lm3 &lt;- update(swiss.lm2, ~. + Catholic)\n\n# Modelle vergleichen\nanova(swiss.lm1, swiss.lm2, swiss.lm3)\n\n\n  \n\n\n\n\n\n\n\n\n\nTipp\n\n\n\nMithilfe der Funktion update() können wir uns sparen, das bisherige Modell noch mal komplett einzutippen. Sie funktioniert nach folgendem Prinzip:\n\nupdate(old_model, new_model)\n\nMit ~. kürzen wir das alte Modell ab, danach ergänzen wir neue Prädiktoren und/oder Interaktionen.\n\n\nDie Funktion anova(x) ist potentiell verwirrend: Wir rechnen hier offensichtlich keine ANOVA, wie wir sie weiter oben kennengelernt haben. Wir führen allerdings schon eine “Analyse der Varianzen” durch - nur beziehen sich die Varianzen auf jeweils die aufgeklärte Varianz der Regressionsmodelle. anova(x) kann als Input nur “fitted model objects” verwerten, also bereits erstellte Modelle, und gibt als Output einen Vergleich dieser Modelle.\nAnsonsten lassen sich Messwiederholungen genauso wie in der “Formelschreibweise” der ANOVA über + Error(ProbandenID/messwiederholteVariable) spezifizieren.\nInteraktionen können wir in die Formel durch * oder : einbauen: Fertility ~ Education*Catholic",
    "crumbs": [
      "Home",
      "Model",
      "Model"
    ]
  },
  {
    "objectID": "model.html#ressourcen",
    "href": "model.html#ressourcen",
    "title": "Model",
    "section": "Ressourcen",
    "text": "Ressourcen\n\n\n\nVerwirrter und frustrierter Student\n\n\nGanz schön viel? Glücklicherweise gibt es im Internet eine Vielzahl extrem hilfreicher Websites, auf denen wir uns informieren können:\n\nÜberblick über rstatix-Funktionen\nListe von Datasets, die direkt in base R abrufbar sind",
    "crumbs": [
      "Home",
      "Model",
      "Model"
    ]
  },
  {
    "objectID": "visualise.html",
    "href": "visualise.html",
    "title": "Visualise: ggplot",
    "section": "",
    "text": "Aufgeregte Studentin, die gerade Daten mit ggplot visualisiert",
    "crumbs": [
      "Home",
      "Visualise",
      "Visualise"
    ]
  },
  {
    "objectID": "visualise.html#einleitung",
    "href": "visualise.html#einleitung",
    "title": "Visualise: ggplot",
    "section": "Einleitung",
    "text": "Einleitung\nWir wissen nun, wie wir Daten in R importieren, bereinigen und transformieren können. Vor allem mithilfe von dplyr können wir schnell & effizient eine bessere Idee über unsere Daten bekommen. Doch tabellarische Daten und einzelne Werte sind nur eine Seite der Data-Science Medaille: Besonders durch Visualisierung bekommen wir eine Idee über die Verteilung von Daten und können versteckte Signale in den Daten enthüllen. Dazu eignet sich ggplot2 hervorragend. Ebenso wie dplyr gehört es zum tidyverse und zu den beliebtesten R-Packages.\nggplot2 sollte sich automatisch auch durch die Aktivierung von tidyverse aktivieren:\n\nlibrary(tidyverse)",
    "crumbs": [
      "Home",
      "Visualise",
      "Visualise"
    ]
  },
  {
    "objectID": "visualise.html#base-r",
    "href": "visualise.html#base-r",
    "title": "Visualise: ggplot",
    "section": "Base R",
    "text": "Base R\nBase-R bietet natürlich auch Funktionen zur Visualisierung von Daten. Jedoch ist es auch hier hilfreich, gleich den “state-of-the-art” zu lernen und sich ggplot2 anzueignen, da wir somit viel mehr Anpassungsmöglichkeiten und eine vereinfachte Grammatik haben.\nTrotzdem wollen wir an dieser Stelle kurz zeigen, dass auch Base-R schnell visualisieren kann. Ein Beispiel dafür ist die hist(x) Funktion, mit der ihr schnell ein Histogramm erstellen könnt, und dessen Syntax wie folgt ist:\n\nhist(dataframe$variable)\n\nTesten wir das Ganze an unserem iris Datensatz. Falls dieser nicht mehr im Environment ist, lesen wir ihn kurzerhand neu ein:\n\niris &lt;- read_csv(\"assets/datasets/iris/iris.csv\")\n\nNun erstellen wir ein Histogramm für die Variable sepal_length:\n\nhist(iris$sepal_length)\n\n\n\n\n\n\n\n\nVoila! Eine Zeile Code, und wir können uns direkt die Verteilung der Daten in Form eines Histogramms anschauen. Aus den oben genannten Gründen wollen wir an dieser Stelle aber den Fokus vor allem auf ggplot2 setzen.\n\n\n\n\n\n\nHinweis\n\n\n\nggplot2 ist der offzielle Name und auch das richtige Package, der Einfachheit halber werden wir aber von ggplot sprechen.",
    "crumbs": [
      "Home",
      "Visualise",
      "Visualise"
    ]
  },
  {
    "objectID": "visualise.html#ggplot",
    "href": "visualise.html#ggplot",
    "title": "Visualise: ggplot",
    "section": "ggplot",
    "text": "ggplot\nGehen wir nun also genauer auf die Syntax von ggplot ein, welche sich etwas von der von der Visualisierung in Base-R unterscheidet.\n\nSyntax\nEbenso wie der Pipe-Operator %&gt;% bei dplyr eine besondere Rolle spielt, tut es das + bei ggplot. Aber dazu gleich mehr. Im Folgenden sehen wir den grundsätzlichen Aufbau eines Befehls bzw. einer Befehlskette, die ein Plot erzeugt:\n\nggplot(data=object)+\n  geom_function(aes(x=variable),parameters=\"xyz\")+\n  additional_functions(parameters=\"additionals\")\n\nDas sieht auf den ersten Blick erstmal komplizierter aus, als es ist. Aber wir werden den Code Schritt für Schritt aufschlüsseln:\n\n\n\n\n\n\nSyntax\n\n\n\n\nPlus Operator +\nWie bereits erwähnt, spielt der + Operator eine wichtige Rolle bei ggplot. Durch + können wir Befehle miteinander verketten. Wichtig zu verstehen ist, dass - ähnlich wie bei dplyr - das Verketten von Befehlen dem “Stapeln” verschiedener Visualisierungen bzw. Eigenschaften entspricht, und jeder einzelne Schritt etwas zum finalen Plot beiträgt. Wie genau das abläuft, werden wir gleich am Beispiel des iris Datensatzes sehen.\n\n\nGeometrische Funktionen geom_function(x)\ngeom_function(x) (geom_ für geometrical) ist hier nur ein Platzhalter für eine Vielzahl verschiedener Funktionen, die wir in Abhängigkeit davon benutzen, was wir visualisieren wollen. So gibt es etwa geom_bar(x) für Bar-Charts, geom_point(x) für Punktwolken oder geom_line(x) für Liniendiagramme. Weiter unten werden wir einige davon beispielhaft vorstellen.\n\n\nAesthetics aes(x)\naes(x) steht für aesthetics und sorgt dafür, Datenpunkte in visuelle Darstellung zu übertragen bzw. zu mappen. Das klingt erstmal abstrakter, als es eigentlich ist. Meistens werden hier die Variablen für die entsprechenden Achsen angebenen (bspw. x=sepal_width) oder aber auch für die Farben bzw. Füllungen von Balken/Punkten (bspw. color=species). Warum das wichtig ist und was der Unterschied zu Parametern außerhalb der aes(x) Funktion ist, werden wir gleich sehen.\n\n\nParameter parameters\nWie bei allen Funktionen können wir hier auch diverse Dinge als Parameter übergeben. Beispiele sind etwa color oder size.\n\n\nZusätzliche Funktionen additional_functions(x)\nAuch additional_functions(x) sind hier nur ein Platzhalter für eine Vielzahl weiterer Funktionen, die wir mit in unsere ggplot Kette nehmen können, bspw. theme_classic(x) oder xlab(x). Mithilfe dieser können wir das Plot im Nachhinein weiter anpassen.\n\n\n\n\n\nBeispiel\nSoweit die Syntax. Am besten lässt sich ggplot aber anhand eines Beispiels verstehen. Machen wir also mit unserem iris Datensatz weiter. Wir wollen die Verteilung der Blattgrößen besser verstehen, und entscheiden uns dazu, ein Plot zu erstellen. Wie bereits beschrieben, hilft der + Operator dabei, verschiedene Funktionen für das Plot miteinander zu verketten. Fangen wir also mit der ersten Ebene an:\n\nggplot(data = iris)\n\n\n\n\n\n\n\n\nWir sehen, mit dem Befehl ggplot(x) haben wir ein leeres Plot erzeugt. Fügen wir nun also ein Diagramm unserer Wahl hinzu. Wir fangen mit geom_bar(x) an.\n\nggplot(data = iris)+\n  geom_bar()\n\nWie wir sehen können, wirft R den Fehler :\n`stat_count()` requires an x or y aesthetic.\nDas liegt daran, dass wir die aes(x) Funktion vergessen haben. Ohne das mapping von Datenpunkten weiß die geom_bar(x) Funktion nicht, wo welcher Datenpunkt hinsoll. Versuchen wir es also nochmal, und spezifizieren wir sepal_width als x-Variable:\n\nggplot(data=iris)+\n  geom_bar(aes(x=sepal_width))\n\n\n\n\n\n\n\n\nWie wir sehen, haben wir nun die Variable sepal_width erfolgreich visualisiert und entdecken eine Normalverteilung.\nDa es sich bei geom_bar() um ein Histogramm handelt, müssen wir lediglich die x-Variable übergeben, da für die y-Variable automatisch gezählt wird.\nAngenommen, wir möchten die Farbe verändern, sodass die Balken die Farbe hellrot haben. Dann müssen wir dies als fill Parameter der geom_bar() Funktion übergeben:\n\nggplot(data=iris)+\n  geom_bar(aes(x=sepal_width), fill=\"lightblue\")\n\n\n\n\n\n\n\n\nNun sind alle Balken rot. Was ist aber, wenn wir wollen, dass die Balken in Abhängigkeit einer Variable verschiedenfarbig sind? Dann müssen wir die fill Variable in der aes(x) Funktion übergeben:\n\nggplot(data=iris)+\n  geom_bar(aes(x=sepal_width, fill=species))\n\n\n\n\n\n\n\n\nPerfekt! Auf einen Blick können wir sehen, dass Pflanzen der Spezies setosa eher größere sepal_width haben. Bis jetzt haben wir allerdings nur einen Befehl mithilfe des + Operators verknüpft. Angenommen, uns stören die Achsenbeschriftungen und wir wollen einen Titel haben. Auch das ist einfach möglich über eine Verkettung mit +:\n\nggplot(data=iris)+\n  geom_bar(aes(x=sepal_width, fill=species))+\n  ylab(\"Prevalence\")+\n  xlab(\"Width of Sepal\")+\n  ggtitle(\"Sepal width of Iris Flower for each species\")\n\n\n\n\n\n\n\n\nDie Farben sehen noch etwas langweilig aus. Ein kurzer Blick auf coolors.co inspiriert uns und wir wollen nun die folgende Farbpalette als Grundlage für unser Plot nehmen:\n\n\n\n\n\n\n\n\n\n\n\nAufgabe\n\n\n\nKopiert die Farbcodes von der Website und erzeugt einen Vektor metro_colors\n\n\n\nmetro_colors &lt;- c(\"#fccc0a\",\"#ff6319\",\"#ee352e\",\"#b933ad\",\"#00933c\",\"#0039a6\",\"#996633\",\"#000000\") \n\nWenn wir nun wollen, dass unser Plot diese Farben hat, können wir eine der scale_xxx_manual(x) Funktionen nehmen. In unserem Fall haben wir oben die Farben für den fill der Bars definiert, daher brauchen wir auch die scale_fill_manual(x) Funktion. Die Syntax ist folgende:\n\nggplot(data=object)+\n  geom_function(aes(x=variable),parameters=\"xyz\")+\n  scale_fill_manual(values = color_vector)\n\ncolor_vector bezieht sich hier auf einen Vektor mit Farbcodes als character eingespeichert. Wir könnten die Farbcodes aber auch direkt als character Vektor übergeben. Neben scale_fill_manual(x) gibt es noch viele weitere Funktionen, wie bspw. scale_color_manual(x) für den colour des Plots.\n\n\n\n\n\n\nAufgabe\n\n\n\nFärbt eure Bars entsprechend der Farben im metro_colors Vektor\n\n\n\nggplot(data=iris)+\n  geom_bar(aes(x=sepal_width, fill=species))+\n  ylab(\"Prevalence\")+\n  xlab(\"Width of Sepal\")+\n  ggtitle(\"Sepal width of Iris Flower for each species\")+\n  scale_fill_manual(values = metro_colors)\n\n\n\n\n\n\n\n\nSuper! Wir hätten nun gerne noch einen schwarzen Rand um die Balken. Dabei hilft uns der Parameter colour in geom_bar():\n\nggplot(data=iris)+\n  geom_bar(aes(x=sepal_width, fill=species), colour=\"#2b2b2b\")+\n  ylab(\"Prevalence\")+\n  xlab(\"Width of Sepal\")+\n  ggtitle(\"Sepal width of Iris Flower for each species\")+\n  scale_fill_manual(values = metro_colors)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFrage\n\n\n\nWarum haben wir hier colour außerhalb des aes(x) Calls definiert?\n\n\nDa wir hier den colour immer dunkelgrau haben wollen (anstatt in Abhängigkeit einer Variable) definieren wir es außerhalb des aes(x) Calls. Jetzt stört uns nur noch der Hintergrund, und dann sind wir zufrieden. ggplot bietet verschiedene themes, die wir einfach über den + Operator verwenden können. Wir entscheiden uns für theme_minimal():\n\nggplot(data=iris)+\n  geom_bar(aes(x=sepal_width, fill=species), color=\"#2b2b2b\")+\n  ylab(\"Prevalence\")+\n  xlab(\"Width of Sepal\")+\n  ggtitle(\"Sepal width of Iris Flower for each species\")+\n  scale_fill_manual(values = metro_colors)+\n  theme_minimal()\n\n\n\n\n\n\n\n\nSuper! Ein publizierfähiges Plot in 7 Zeilen wiederverwendbarem Code. An diesem Beispiel haben wir gesehen, wie einfach das mit ggplot und das verketten von Befehlen geht. Im Folgenden wollen wir euch noch eine Auswahl an Visualisierungsfunktionen, sogennante geom_() vorstellen, die wir häufig benötigen.",
    "crumbs": [
      "Home",
      "Visualise",
      "Visualise"
    ]
  },
  {
    "objectID": "visualise.html#visualisierungsfunktionen",
    "href": "visualise.html#visualisierungsfunktionen",
    "title": "Visualise: ggplot",
    "section": "Visualisierungsfunktionen",
    "text": "Visualisierungsfunktionen\nWie bereits beschrieben, sind geom_ die Visualisierungsfunktionen. Jenachdem, wie unsere Daten strukturiert sind, bietet sich einige davon mehr, andere weniger an. Eine sinnvolle Einteilung geht von den Achsen bzw. zu visualisierenden Variablen aus. Dabei sollten wir uns immer die Frage stellen:\n\nWieviele Variablen möchte ich visualieren?\n\nIn einem 2-dimensionalen Koordinatensystem können wir natürlich erstmal nur zwei Variablen darstellen. Allerdings erlaubt uns ggplot auch die Visualisierung von mehr Variablen. So können wir etwa über fill bzw. colour weitere Variablen mit in unser Plot einbringen. Eine weitere Frage, die wir uns in diese Richtung stellen sollten, ist:\n\nWas für einen Datentyp haben die zu visualisierenden Variablen?\n\nJe nachdem, wie die wir diese Fragen beantworten, bieten sich dann verschiedene Funktionen an. Im obigen Beispiel haben wir eine Variable (sepal_width), welche intervallskaliert ist. Dafür bietet sich entsprechend ein Histogramm bzw. geom_bar(x)an.\n\n\n\n\n\n\nFrage\n\n\n\nWas für Visualisierungen kommen bei zwei intervallskalierten Variablen in Frage?\n\n\nRichtig, hier bietet sich vor allem ein Scatterplot an, in ggplot heißt das geom_point(x). Immer wenn es darum geht, eine Übersicht über die Funktionen eines Packages zu bekommen, können die Cheatsheets hilfreich sein. Besonders zur Inspiration und zum Brainstormen zu einer Visualisierung bietet sich hierbei das Cheatsheet zu ggplot an:\n\n\n\n\n\nDort sind die Vielzahl weiterer Visualisierungen zu finden. Nebem geom_bar(x) wollen wir in dieser Session noch zwei weitere, häufig benötigte Funktionen sowie deren Kombination zeigen: geom_point(x) und geom_smooth(x).\n\nScatterplot - geom_point(x)\nAngenommen, wir haben zwei intervallskalierte Variablen, die wir visualisieren wollen. Dann bietet sich geom_point(x) an. In unserem Beispiel des iris Datensatzes ist das der Fall - hier sind die Variablen sepal_width etc. intervallskaliert. Wollen wir uns also anschauen, wie das Ganze in einem Scatterplot aussieht, so können wir die geom_point(x) Funktion verwenden.\n\n\n\n\n\n\nAufgabe\n\n\n\nVisualisiert sepal_width und sepal_length als Scatterplot.\n\n\nVersuchen wir also mal, die Verteilung von sepal_width und sepal_length zu visualisieren. Wir übernehmen die Anpassungen (Achsenbeschriftung etc.) von oben.\n\nggplot(data=iris)+\n  geom_point(aes(x=sepal_width,y=sepal_length))+\n  ylab(\"Length of Sepal\")+\n  xlab(\"Width of Sepal\")+\n  ggtitle(\"Sepal width and length of Iris Flower\")+\n  theme_minimal()\n\n\n\n\n\n\n\n\nSuper! Nun wäre es praktisch, wenn wir auch noch die Variable species einbringen könnten. Dies können wir durch colour in aes() tun.\n\n\n\n\n\n\nAufgabe\n\n\n\nBringt nun auch die Farbe ein und benutzt dafür die metro_colors Palette\n\n\n\nggplot(data=iris)+\n  geom_point(aes(x=sepal_width,y=sepal_length,color=species))+\n  scale_color_manual(values=metro_colors)+\n  ylab(\"Length of Sepal\")+\n  xlab(\"Width of Sepal\")+\n  ggtitle(\"Sepal width and length of Iris Flower for each species\")+\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipp\n\n\n\nWir verwenden hier colour anstatt fill, da in geom_point(x) die Farbe der Punkte über color anstatt fill bestimmt wird.\n\n\nAuch hier sehen wir direkt, dass es einen Unterschied zwischen den verschiedenen Spezies zu geben scheint. Visuell können wir auch den (offensichtlichen) Zusammenhang zwischen sepal_width und sepal_length erkennen.\n\n\nRegressionsgrade - geom_smooth(x)\nDa wir Wissenschaftler:innen sind wollen wir sicher gehen und das Ganze noch genauer verstehen, vor allem den Zusammenhang zwischen sepal_width und sepal_length. Dazu können wir uns in den Scatterplot noch eine Regressionsgrade hineinlegen mit Hilfe von geom_smooth(x).\ngeom_smooth(x) hat eine Besonderheit, und zwar die Methode, mit der die Regressionsgrade kreiert wird. Hierbei gibt es die Möglichkeiten lm, glm gam oder loess. Wir entscheiden uns für lm (linear model), um eine lineare Regressionsgrade zu zeichnen. Wer hier genauer nachlesen möchte, der/die tippt ?geom_smooth in die R-Konsole.\nWir können die Funktion über den + Operator tun - denn in ggplot können wir alle Visualisierungsfunktionen nach Belieben “stapeln”:\n\nggplot(data=iris)+\n  geom_point(aes(x=sepal_width,y=sepal_length,color=species))+\n  scale_color_manual(values=metro_colors)+\n  ylab(\"Length of Sepal\")+\n  xlab(\"Width of Sepal\")+\n  ggtitle(\"Sepal width and length of Iris Flower for each species\")+\n  theme_minimal()+\n  geom_smooth(aes(x=sepal_width,y=sepal_length), method=\"lm\") # lm for linear model\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nDie Regressionsgrade zeigt einen negativen Zusammenhang, obwohl wir für die einzelnen Spezies einen positiven Zusammenhang sehen. Das liegt daran, dass geom_smooth(x) unseren Faktor species nicht berücksichtigt.\n\n\n\n\n\n\nFrage\n\n\n\nWoran könnte das liegen?\n\n\nDa im aes(x) Call von geom_smooth(x) color nicht übergeben wurde, berechnet die Funktion die Regressionsgrade unabhängig von der Variable species. Dies können wir ändern, indem wir in dessen aes(x) Funktion ebenfalls colour übergeben.\n\nggplot(data=iris)+\n  geom_point(aes(x=sepal_width,y=sepal_length,color=species))+\n  scale_color_manual(values=metro_colors)+\n  ylab(\"Length of Sepal\")+\n  xlab(\"Width of Sepal\")+\n  ggtitle(\"Sepal width and length of Iris Flower for each species\")+\n  theme_minimal()+\n  geom_smooth(aes(x=sepal_width,y=sepal_length,color=species), method=\"lm\") # lm for linear model\n\n\n\n\n\n\n\n\nVoila! Wir sehen unsere drei kleinen Regressionsgraden und den erwarteten positiven Zusammenhang.\n\n\n\n\n\n\nTipp\n\n\n\nAufmerksame Programmierer:innen horchen hier bei dem doppelten aes(x) call vielleicht auf: “Redundanz!”, könnte man uns vorwerfen. In der Tat ließe sich der obige Code auch so lösen:\n\nggplot(data=iris,aes(x=sepal_width,y=sepal_length,color=species))+\n  geom_point()+\n  scale_color_manual(values=metro_colors)+\n  ylab(\"Length of Sepal\")+\n  xlab(\"Width of Sepal\")+\n  ggtitle(\"Sepal width and length of Iris Flower for each species\")+\n  theme_minimal()+\n  geom_smooth(method=\"lm\") # lm for linear model\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nWir sehen, dass wir auch bei der ggplot(x) Funktion per aes(x) die relevanten Variablen übergeben können. Sollten die für alle aneinandergeketteten geom_(x) Funktionen die gleichen sein, so spart das Platz und macht den Code effizienter.",
    "crumbs": [
      "Home",
      "Visualise",
      "Visualise"
    ]
  },
  {
    "objectID": "visualise.html#plots-speichern",
    "href": "visualise.html#plots-speichern",
    "title": "Visualise: ggplot",
    "section": "Plots speichern",
    "text": "Plots speichern\nDie Frage nach dem Speichern von Plots ist berechtigt, wenngleich wir dies in der Praxis häufig nicht tun. Der häufigste “Modus Operandi” ist das Erzeugen von Plots direkt in Chunks oder in der Konsole. Aus dem Chunk-Output können diese dann herauskopiert werden (rechte Maustaste).\nNoch besser funktioniert es aber, wenn wir den Code für die Erstellung des Plots in die Konsole kopieren, und dort ausführen. Denn dann erscheint das Plot rechts unten unter “Plots”. Hier haben wir viel mehr Möglichkeiten für den Export:\n\nBspw. PDF oder als Bild. Wir präfererien das Speichern als pdf, da wir so eine Vektorgrafik haben welche wir im Nachhinein noch für das Fine-Tuning mit Adobe Illustrator bearbeiten können.",
    "crumbs": [
      "Home",
      "Visualise",
      "Visualise"
    ]
  },
  {
    "objectID": "visualise.html#fazit-weitere-resourcen",
    "href": "visualise.html#fazit-weitere-resourcen",
    "title": "Visualise: ggplot",
    "section": "Fazit & Weitere Resourcen",
    "text": "Fazit & Weitere Resourcen\nWir haben nun gelernt, wie wir mithilfe von ggplot Daten visualisieren können. Natürlich war das nur die Spitze des Eisbergs, und noch viel mehr ist möglich. Einige dieser Dinge werden wir im weiteren Verlauf des Seminars kennenlernen, wenn es um das Arbeiten (und Visualisieren) von konkreten Datensätzen geht.\nNeben dieser Website hier (🥳) gibt es natürlich noch viele weitere, tolle Resourcen im Internet. Neben den offziellen Dokumentationen bspw. auf tidyverse oder CRAN wollen wir euch vor allem mit Blick auf dplyr und ggplot noch einige ans Herz legen:\n\nsthda.com: Be Awesome in ggplot2: A Practical Guide to be Highly Effective - R software and data visualization\nr-graph-gallery.com: A collection of charts made with the R programming language\nr-bloggers.com: DPLYR: A beginner’s guide",
    "crumbs": [
      "Home",
      "Visualise",
      "Visualise"
    ]
  },
  {
    "objectID": "visualise.html#nächste-session",
    "href": "visualise.html#nächste-session",
    "title": "Visualise: ggplot",
    "section": "Nächste Session",
    "text": "Nächste Session\nDas war’s mit dieser Session. In der nächsten Session geht es um Modelling - wie können wir unsere Erkenntnisse denn nun statistisch absichern?",
    "crumbs": [
      "Home",
      "Visualise",
      "Visualise"
    ]
  },
  {
    "objectID": "import.html",
    "href": "import.html",
    "title": "Import",
    "section": "",
    "text": "Studentin importiert Daten mit R und sieht, wie einfach das ist.",
    "crumbs": [
      "Home",
      "Import",
      "Import"
    ]
  },
  {
    "objectID": "import.html#einleitung",
    "href": "import.html#einleitung",
    "title": "Import",
    "section": "Einleitung",
    "text": "Einleitung\nDaten können in einer Vielzahl von Formaten vorliegen. Sie können entweder unstrukturiert sein (bspw. Bilder), oder strukturiert (bspw. tabellenbasiert). Für unsere Forschungszwecke sind Daten meistens bereits strukturiert (bspw. Proband:innendaten), was das Einlesen in R sehr einfach macht.\nDoch auch bei strukturierten Daten können diese entweder als Excel Dateien (.xlsx), SPSS-Dateien (.sav) oder Textdateien (.txt oder .csv) vorliegen.\n\n\n\n\n\nKein Problem! R kann mit allen diesen Datenformaten umgehen. In dieser Session lernen wir, wie wir verschiedene Datenformate auf verschiedene Weisen in R importieren können. Vorher aber noch ein kurzer Hintergrund zu (Text)daten.",
    "crumbs": [
      "Home",
      "Import",
      "Import"
    ]
  },
  {
    "objectID": "import.html#hintergrund-textdaten",
    "href": "import.html#hintergrund-textdaten",
    "title": "Import",
    "section": "Hintergrund: Textdaten",
    "text": "Hintergrund: Textdaten\nDie meisten strukturierten Daten, die uns als Forschende interessieren, sind im Kern Textdaten. Etwaige Dateiformate wie bspw. .xlsx sind um zusätzliche Funktionen angereicherte Dateiformate. In R interessieren uns allerdings meistens nur die Daten - und da wir mit den tollen Funktionen von R auch alles andere machen können, brauchen wir die meisten dieser Zusatzfunktionen gar nicht.\n\nTXT & CSV\nEin Beispiel für Textdaten in ihrer Reinform sind .txt und .csv. Während .txt-Dateien unformatierte Textdateien sind welche beliebigen Text speichern, sind .csv-Dateien (Comma-Separated Values) speziell für die Speicherung tabellarischer Daten konzipiert und für uns besonders interessant. Wenn wir eine .csv Datei mit einem Texteditor öffnen, sieht sie wie folgt aus:\n\n\n\nEine .csv Datei geöffnet in einem Texteditor. Wir können sehen, wie die einzelnen Werte durch Kommata getrennt sind und die erste Zeile die Variablennamen beschreibt\n\n\nDas tolle hierbei ist, dass uns diese Form der Datenspeicherung für die allermeisten Anwendungsfälle, die wir in R haben, völlig ausreicht. Daher werden wir im Rahmen dieses Seminar auch stets mit .csv Dateien arbeiten.",
    "crumbs": [
      "Home",
      "Import",
      "Import"
    ]
  },
  {
    "objectID": "import.html#einlesen-von-daten",
    "href": "import.html#einlesen-von-daten",
    "title": "Import",
    "section": "Einlesen von Daten",
    "text": "Einlesen von Daten\nNun wollen wir uns anschauen, wie genau wir Daten in R einlesen können. Da uns im Alltag durchaus auch andere Daten als .csv begegnen können, wollen wir uns im Folgenden anhand der drei Formate .csv, .xlsx und .sav anschauen, wie wir sie in R importieren können. Dazu verwenden wir einen in der Data Science community sehr beliebten Datensatz, welcher simpel ist, anhand dessen wir aber trotzdem gut die Konzepte verstehen können.\n\nIris Datensatz\nDer Iris Datensatz ist ein berühmter Datensatz von R.A. Fisher aus dem Jahr 1936, in dem verschiedene Eigenschaften in Bezug auf die Blütengröße verschiedener Spezies der Blume Iris enthalten sind (siehe hier).\n\n\n\nSources: Wikipedia; Danielle Langlois, Денис Анисимов & Eric Hunt\n\n\nIm Zentrum des Datensatzes steht die Größe der verschiedenen Blatttypen für die jeweilige Spezies. Dabei gibt es die Blatttypen Sepal und Petal. Die folgende Grafik verdeutlicht das etwas:\n\n\n\n\n\nDer Datensatz lässt sich im folgenden in den drei relevanten Formaten herunterladen:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHinweis\n\n\n\nUm die .csv Datei herunterzuladen, klickt ihr mit der rechten Maustaste auf den Link und klickt auf Verknüpfte Datei speichern unter\n\n\n\n\nBenötigte Packages\nDa Base-R den Import von xlsx und .sav Dateien nicht nativ unterstützt, müssen wir zwei Packages installieren.\n\nHaven (haven)\n\nImport von SPSS-Daten wie .sav samt Variablenlabels\n\nReadxl (readxl)\n\nImport von Excel-Dateien\n\n\nDa beide Packages bereits im tidyverse Package enthalten sind, brauchen wir sie nicht zu installieren, müssen sie allerdings noch aktivieren (bei kleineren Packages ist das manchmal der Fall).\n\n\n\n\n\n\nAufgabe\n\n\n\nAktiviert die beiden Packages.\n\n\n\nlibrary(haven)\nlibrary(readxl)\n\n\n\nCSV\nBeginnen wir mit dem einfachsten Format, für das wir gar nicht zwangsläufig ein Package brauchen. Beim importieren hilft die Base-R Funktion read.csv(x) bzw. die read_csv(x) Funktion aus dem readr Package in tidyverse, welche ein paar Vorteile hat. Dazu gehört under anderem die automatische Erkennung des Trennzeichens (bei deutschen Excel-Versionen wird bspw. aufgrund des Dezimalzeichens mit dem Komma statt dem Punkt als Trennzeichen das Semikolon ; verwendet), weshalb wir hier die read_csv(x) Funktion verwenden. Der Import funktioniert nach folgender Syntax:\nobject &lt;- read_csv(\"path/to/file/\")\nWobei object dem Namen entspricht, dem wir unserem importiertem Datensatz geben wollen. Fangen wir also an:\n\niris_csv &lt;- read_csv(\"assets/datasets/iris/iris.csv\")\n\nFür den Dateipfad könnt ihr entweder den absoluten Pfad nehmen, oder falls die Datei direkt “neben” eurer .qmd Datei liegt, reicht der Dateiname ohne zusätzliche Pfadinformationen aus, also etwa so:\n\niris_csv &lt;- read_csv(\"iris.csv\")\n\n\n\n\n\n\n\nTipp\n\n\n\nMac User aufgepasst: Ihr könnt euch den absoluten Dateipfad ganz leicht per Rechtsklick auf eine Datei bei gedrückter OPTION Taste in die Zwischenablage kopieren.\n\n\nVoilá! Unser Datensatz aus .csv ist eingelesen.\n\n\nXLSX\nFür den Import von Excel (.xlsx) Dateien brauchen wir das readxl Package, welches wir bereits installiert und aktiviert haben. Die entsprechende Funktion ist die read_excel(x) Funktion. Die Syntax ist hier die gleiche.\n\niris_xlsx &lt;- read_excel(\"assets/datasets/iris/iris.xlsx\")\n\nDas war einfach! Und wie sieht es mit .sav Dateien aus?\n\n\nSAV\nAuch hier ist es dank des haven Packages sehr einfach:\n\niris_sav &lt;- read_sav(\"assets/datasets/iris/iris.sav\")\n\n\n\n\n\n\n\nFrage\n\n\n\nSo manch ein vertrauter SPSS-Nutzer mag jetzt fragen: Und was geschieht mit meinen Variablenlabels? Auch darauf hat R natürlich eine Antwort. Das haven Package unterstützt dies, wir kommen aber zu einem späteren Zeitpunkt im Seminar nochmal dazu.\n\n\nAm Ende sollten alle drei Dataframes in unserem Environment auftauchen, und gleich aussehen:\n\nWir können das Ganze noch über die summary(x) Funktion testen um zu schauen, wie der Datensatz aussieht:\n\nsummary(iris_csv)\n\n  sepal_length    sepal_width     petal_length    petal_width   \n Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100  \n 1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300  \n Median :5.800   Median :3.000   Median :4.350   Median :1.300  \n Mean   :5.843   Mean   :3.054   Mean   :3.759   Mean   :1.199  \n 3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800  \n Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500  \n   species         \n Length:150        \n Class :character  \n Mode  :character",
    "crumbs": [
      "Home",
      "Import",
      "Import"
    ]
  },
  {
    "objectID": "import.html#einlesen-per-gui",
    "href": "import.html#einlesen-per-gui",
    "title": "Import",
    "section": "Einlesen per GUI",
    "text": "Einlesen per GUI\nWir können nun Daten in R einlesen. Der Vollständigkeit halber möchten wir aber auch noch zeigen, wie man mit R-Studio auch per GUI Daten einlesen kann. Dazu gehen wir zu File/Import Dataset/From Text (readr):\n\nWenn wir nun eine Datei ausgewählt haben, öffnet sich folgendes Fenster:\n\nDort können wir verschiedene Einstellungen vornehmen, vieles erkennt R-Studio aber schon automatisch. Unten rechts befindet sich Code Preview - alle unsere Aktionen werden also ohnehin intern als Code ausgeführt. Ein Klick auf Import importiert dann den Datensatz und schreibt ihn in unser Environment, so wie oben.\n\n\n\n\n\n\nAchtung\n\n\n\nZwar können wir so vorgehen, das schöne an R ist aber, dass wir durch unseren Code eine Nachvollziehbarkeit gewährleisten können - vom Import bis zum Bericht, also entlang der gesamten Pipeline. Manuelle Importaktionen etc. können diese Nachvollziehbarkeit nicht mehr gewährleisten.\n\n\n\nNächste Session\nDas war’s mit dieser Session. In der nächsten Session geht es um Wrangling mit Base R.",
    "crumbs": [
      "Home",
      "Import",
      "Import"
    ]
  },
  {
    "objectID": "installation.html",
    "href": "installation.html",
    "title": "Installation",
    "section": "",
    "text": "title: “Installation” author: “Simon Krukowski” format: html\n\n\n\n\nStudentin die gerade R und R-Studio herunterlädt\n\n\n\nInstallation\nIm Folgenden erklären wir, wie ihr euch R und RStudio herunterladen könnt. Zwar gibt es im Web einige How-Tos und Tutorials, wir wollen euch an dieser Stelle der Vollständigkeit halber aber nochmal vermitteln, wie genau ihr euch die Software installiert und was es dabei zu beachten gibt. Falls während der Installation Fragen aufkommen, meldet euch gerne per Mail bei mir (simon.krukowski@uni-due.de) oder schaut einfach im Web nach How-Tos/Troubleshooting. Das ist generell auch eine gute Idee beim Thema R, Programmieren & Datenanalyse - mehr dazu erfahrt ihr dann im Seminar.\n\n\nÜberblick\nDa dein Computer R als Programmiersprache nicht von selbst versteht, musst du es zunächst installieren. Nach der Installation kannst du es theoretisch benutzen und Daten damit auswerten. Hierbei ist wichtig zu differenzieren, dass du R zwar in der Kommandozeile/Terminal verwenden kannst, dies aber wenig intuitiv ist. Im folgenden Video siehst du, wie es aussehen kann, R in der Kommandozeile zu benutzen, nachdem du es installiert hast:\n\nWenn es nun aber um komplexere Rechenoperationen als 1+1, das Einlesen von Datensätzen oder das Generieren von Grafiken geht, wird es schwierig mit der Kommandozeile. Daher bietet es sich an, eine IDE (Entwicklungsumgebung, siehe hier) zu benutzen. Die populärste IDE dafür ist RStudio:\n\nIm Rahmen dieses Seminars werden wir mit RStudio arbeiten. Wie du beides installierst, erfährst du in dieser Lektion.\n\n\nDownload & Installation\nStarten wir also mit dem Download. Besuche folgende Website:\n\n\n\n\n\nWenn du dem Link folgst, wirst du auf die offzielle Seite von R (CRAN) weitergeleitet, und kannst dir dort die für dein System richtige Version auswählen:\n\n\n\n\n\nIn unserem Fall haben wir einen Mac, und wählen den entsprechenden Link aus. Auf der folgenden Seite gibt es viele verschiedene Links, hier ist es wichtig den bei latest release und für euer System richtigen zu nehmen (in unserem Fall Apple Silicon).\n\n\n\n\n\nNach dem Download könnt ihr den Installer ausführen und voilà - ihr habt R installiert! Da bei uns R schon installiert ist gibt es hier leider keinen Screenshot.\nTheoretisch könntet ihr R nun wie oben beschrieben in der Kommandozeile verwenden. Wir wollen allerdings RStudio benutzen, also gehen wir wieder zurück zu posit.co und laden dort die richtige Version für unser System herunter (die richtige Version wird hier automatisch vorgeschlagen):\n\n\n\n\n\nJe nach System unterscheidet sich hier die Installation etwas, aber am Ende sollte RStudio bei dir installiert sein. Öffnet also R, und es sollte etwa wie folgt aussehen:\n\n\n\n\n\n\n\nR-Skripte ausführen\nAls erste Übung wollen wir ein Test-Skript ausführen. Ladet euch das Skript herunter und bringt es bei der ersten Seminarstunde mit. Dort werden wir das Ergebnis besprechen.\n\n\n R-Skript herunterladen\n\n\n\n\nTroubleshooting\nHaben sich bei der Installation Probleme ergeben? Meldet euch gerne per Mail an simon.krukowski@uni-due.de.",
    "crumbs": [
      "Home",
      "Intro",
      "Installation"
    ]
  },
  {
    "objectID": "pipeline.html",
    "href": "pipeline.html",
    "title": "Pipeline",
    "section": "",
    "text": "Unser Seminar und diese Website orientieren sich an der R-Datenanalyse Pipeline (In Anlehnung an Wickham, Çetinkaya-Rundel, and Grolemund (2023)). Wir lernen also alle nötigen Schritte, um R zu meistern - von Datenimport über Bereinigung und Aufbereitung hin zu Visualisierung und Modellierung. Natürlich lernen wir auch, wie wir das Ganze dann kommunizieren.\nDie verschiedenen “Stationen” dieser Pipeline sind nicht zwingend konsekutiv - d.h. bspw. vor allem im Explore Teil durchlaufen wir die verschiedenen Stationen mehrfach, um Erkenntnisse zu gewinnen. Um mehr Übersichtlichkeit zu gewährleisten, bilden diese Stationen auch thematischen Oberkategorien dieser Veranstaltung.\n\n\n\nR-Datenanalysepipeline angelehnt an Wickham, Çetinkaya-Rundel, and Grolemund (2023)\n\n\n\n\n\n\nReferences\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023. R for data science: import, tidy, transform, visualize, and model data. 2nd edition. Beijing Boston Farnham Sebastopol Tokyo: O’Reilly.",
    "crumbs": [
      "Home",
      "Intro",
      "Pipeline"
    ]
  }
]